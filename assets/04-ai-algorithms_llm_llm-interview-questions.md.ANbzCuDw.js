import{_ as i,c as t,o as p,ag as o}from"./chunks/framework.DT5BmYxR.js";const m=JSON.parse('{"title":"大模型的一些面试题小结","description":"","frontmatter":{},"headers":[],"relativePath":"04-ai-algorithms/llm/llm-interview-questions.md","filePath":"04-ai-algorithms/llm/llm-interview-questions.md"}'),r={name:"04-ai-algorithms/llm/llm-interview-questions.md"};function e(n,l,s,a,g,c){return p(),t("div",null,[...l[0]||(l[0]=[o('<h1 id="大模型的一些面试题小结" tabindex="-1">大模型的一些面试题小结 <a class="header-anchor" href="#大模型的一些面试题小结" aria-label="Permalink to &quot;大模型的一些面试题小结&quot;">​</a></h1><p><strong>Q1：仅编码器（BERT类）、仅解码器（GPT类）和完整的编码器-解码器架构各有什么优缺点？</strong></p><p><img src="https://picx.zhimg.com/v2-290edb528ef0c76433de37f78f1fdfd3_1440w.jpg" alt="img"></p><p><strong>Q2：<a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;zhida_source=entity" target="_blank" rel="noreferrer">自注意力机制</a>如何使大模型能够捕捉长距离依赖关系，它跟RNN有什么区别？</strong></p><p>自注意力机制允许序列中<strong>每个位置直接与所有其他位置交互</strong>，通过计算 Query 和 Key 的相似度，对全局信息进行加权聚合。因此，无论依赖关系有多远，都能在一次计算中捕捉到。</p><p>相比之下，RNN 是<strong>一步步传递信息</strong>，远距离依赖需要多步传播，容易造成<strong>梯度消失</strong>，也限制了模型对长依赖的建模能力。</p><p>此外，自注意力机制支持<strong>并行计算</strong>，而 RNN 必须串行执行，训练效率也低很多。</p><p><strong>Q3：大模型为什么有<a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=%E4%B8%8A%E4%B8%8B%E6%96%87%E9%95%BF%E5%BA%A6&amp;zhida_source=entity" target="_blank" rel="noreferrer">上下文长度</a>的概念？为什么它是指输入和输出的总长度？</strong></p><p><a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=Transformer+%E6%A8%A1%E5%9E%8B&amp;zhida_source=entity" target="_blank" rel="noreferrer">Transformer 模型</a>在处理文本时，会把输入 token 编码成一个固定长度的序列，并通过位置编码保留顺序信息。模型的注意力机制是全局的，每个 token 都会与序列中其他 token 进行交互，其计算复杂度是 �(�2) ，这在显存或内存上有较高开销，因此在训练阶段我们就需要设定一个最大的上下文长度，比如 2048 或 4096 个 token。</p><p>而这个“上下文长度”指的是<strong>当前模型可感知的全部 token 的数量</strong>，也就是“输入 token + 已生成 token”的总和。原因是像 GPT 类模型是自回归的，每生成一个新 token，都需要重新读取整个已有上下文作为输入。所以如果总长度超过了模型的最大支持范围，就必须截断或者做缓存处理。</p><p><strong>Q4：大模型的<a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=%E5%88%86%E8%AF%8D%E5%99%A8&amp;zhida_source=entity" target="_blank" rel="noreferrer">分词器</a>和传统的中文分词有什么区别？对于指定的词表，一句话是不是只有唯一的分词方式？</strong></p><p><img src="https://picx.zhimg.com/v2-bee33a7a938bcb7ac1d588cfd19c4223_1440w.jpg" alt="img"></p><p>大模型的分词是为了满足模型输入token的形式，而传统中文分词是为了符合人类语义理解习惯。大模型的分词器在编码阶段，对于一个固定词表，一句话的分词方式是唯一的。因为大模型的分词器会使用固定的词表，并且按照最长匹配优先的规则从前往后一个个切分。这个过程是贪心算法，没有随机性，也不考虑上下文，词表一样，分词规则一样，输入一样，输出就一定一样。</p><p><strong>Q5：大模型是如何区分聊天历史中用户说的话和 AI 说的话的？</strong></p><p>大模型区分用户和 AI，是靠在输入里加明确的角色标记，如 <code>&lt;|im_start|&gt;user</code> 和 <code>&lt;|im_start|&gt;assistant</code>。模型并不理解“谁是谁”，它只是学会了这些格式对应的说话模式。只要格式规范，模型就能正确判断并生成对应角色的回复。</p><p><strong>Q6：传统的<a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=%E9%9D%99%E6%80%81%E8%AF%8D%E5%B5%8C%E5%85%A5&amp;zhida_source=entity" target="_blank" rel="noreferrer">静态词嵌入</a>（如 <a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=word2vec&amp;zhida_source=entity" target="_blank" rel="noreferrer">word2vec</a>）与大模型产生的上下文相关的嵌入相比，有什么区别？有了 与上下文相关的嵌入，静态词嵌入还有什么价值？</strong></p><p><img src="https://pica.zhimg.com/v2-2da5ae8638afd54733749e0aaac52cc6_1440w.jpg" alt="img"></p><p>静态词嵌入的价值：</p><ol><li>轻量、计算快：适合资源受限场景，不需要大型模型推理</li><li>可解释性强：向量空间更易可视化，便于分析词语相似性。</li><li>预训练简单：不依赖上下文，可以用大规模语料快速训练。</li></ol><p><strong>Q7：在 word2vec 等词嵌入空间中，存在 king – man + woman ≈ queen 的现象，这是为什么？大模型 的词元嵌入空间是否也有类似的属性？</strong></p><p>Word2Vec 学到的词向量不仅捕捉了语义相似性，还在向量空间中学到了一些语义关系的方向性。在大模型（如 GPT、BERT）中，词元嵌入层也会保留类似的结构关系，尤其在 embedding 层和初始的表示空间中，可以观测到一些类似的加减逻辑。但因为大模型后续加入了上下文建模，经过多层 Transformer 的非线性变换后，最终的 token 表示是上下文相关的，不一定再保持简单的线性结构。</p><p><strong>Q8：注意力机制是如何计算上下文各个词元之间的相关性的？每个注意力头只关注一个词元吗？</strong></p><p>注意力机制的核心原理是通过动态权重分配，让模型在处理输入数据时能够聚焦于关键信息。其计算过程分为三步：</p><ol><li>相似度计算：用查询向量（Query）与键向量（Key）计算点积，衡量词元间的关联强度，并通过缩放避免数值过大</li><li>权重归一化：对相似度分数应用Softmax，转化为概率分布，突出重要词元</li><li>上下文生成：用归一化权重对值向量（Value）加权求和，得到融合全局信息的表示</li></ol><p>每个注意力头并不只关注一个词元，而是从不同子空间学习多样化的关联模式，一个头可能捕捉语法结构（如主谓一致），另一个头关注关键词匹配，多头并行计算后拼接结果，综合多视角信息。</p><p><strong>Q9：如果需要通过修改尽可能少的参数值，让模型忘记某一特定知识，应该修改注意力层还是前馈神经网络层的参数。</strong></p><p>要让模型快速忘记某个特定知识，优先修改注意力层的参数（尤其是Q/K矩阵），因为注意力层直接控制词元间的关联性，调整少量参数即可切断目标知识的上下文联系。理由如下：</p><ol><li><strong>精准性</strong>：注意力层直接控制词元关联性，调整Q/K矩阵可快速切断目标知识的上下文联系</li><li><strong>高效性</strong>：仅需修改0.1%-1%参数（如冻结特定注意力头或LoRA微调），远少于FFN层</li><li><strong>安全性</strong>：对模型其他功能干扰更小，避免FFN层修改引发的连带遗忘</li></ol><p><strong>Q10：为什么注意力机制需要多个头？跟简单地减少注意力头的数量相比，多查询注意力和分组查询 注意力优化有什么不同？它们优化的是训练阶段还是推理阶段？</strong></p><p>多头设计（MHA）通过并行计算多个注意力头，使模型能同时捕捉序列中不同维度的依赖关系（如语法、语义、长距离关联等），显著提升模型表达能力。</p><p>多查询注意力（MQA）与分组查询注意力（GQA）的优化差异</p><table tabindex="0"><thead><tr><th>特性</th><th>多头注意力（MHA）</th><th>多查询注意力（MQA）</th><th>分组查询注意力（GQA）</th></tr></thead><tbody><tr><td>键值共享</td><td>每个头独立计算Q/K/V</td><td>所有头共享K/V，仅Q独立</td><td>查询头分组，组内共享K/V</td></tr><tr><td>计算效率</td><td>高计算成本（H组K/V）</td><td>最优（1组K/V）</td><td>平衡（G组K/V，G</td></tr><tr><td>精度损失</td><td>无</td><td>高</td><td>适中</td></tr><tr><td>适用阶段</td><td>训练与推理</td><td>主要优化推理阶段</td><td>兼顾训练微调与推理</td></tr></tbody></table><p><strong>Q11：<a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=Flash+Attention&amp;zhida_source=entity" target="_blank" rel="noreferrer">Flash Attention</a> 并不能减少计算量，为什么能提升推理速度？ Flash Attention 是如何实现增量计算 softmax 的？</strong></p><p>Flash Attention 主要通过以下几种方式提升推理速度：</p><ol><li>内存访问优化：传统的注意力计算需要大量的内存访问，尤其是在计算 Query、Key 和 Value 的矩阵乘法时。Flash Attention 使用了更高效的内存访问模式，通过利用 GPU 的高效缓存和内存带宽，减少了不必要的数据传输，特别是在大模型的推理中，大量的内存访问和数据复制会拖慢速度，而 Flash Attention 通过优化内存访问方式，显著加速了计算。</li><li>减少重复计算：传统注意力机制计算时，softmax 和加权求和是独立计算的，Flash Attention 通过增量计算softmax，避免了重复计算，减少了计算量。</li><li>更高效的硬件利用：Flash Attention 通过专为 GPU 设计的高效算法，能够更好地利用 GPU 上的计算资源，减少了计算延迟。</li></ol><p>它将注意力矩阵按块（block）读取，每次只处理一部分 token，在计算 softmax 时，不是一次性对整行做归一化，而是边读边计算最大值与加权和，逐步更新 softmax 的中间结果。这样可以避免保存整个注意力矩阵，用更少的内存完成等价计算，从而提升速度和效率。</p><p><strong>Q12：跟原始 Transformer 论文中的绝对位置编码相比，<a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=RoPE&amp;zhida_source=entity" target="_blank" rel="noreferrer">RoPE</a>（旋转位置嵌入）有什么优点？ RoPE 在长上下文外推时会面临什么挑战？</strong></p><p>RoPE的优势：</p><ul><li>RoPE编码的是“相对位置”，而不是固定的位置编号，所以模型更容易捕捉词和词之间的距离关系。</li><li>RoPE可以自然推广到更长的上下文，因为它是通过旋转角度连续编码位置的，只要角度公式支持，就能处理比训练时更长的文本。</li><li>更适合生成任务，因为它直接作用在注意力计算里，对位置敏感，但不改变 token 本身的表示。</li></ul><p>RoPE 在长上下文外推时的挑战：</p><ul><li>位置越大，旋转角度越大，容易导致表示混乱，模型难以判断远距离 token 的真实关系。</li><li>超出训练长度后，模型没见过这些角度，就会“失去记忆力”，注意力权重变得不准确。</li><li>高频维度变化太快，长距离信息反而被扭曲，模型难以稳定利用上下文。</li></ul><p><strong>Q13：嵌入模型 + 逻辑回归的分类方式获得了 0.85 的 F1 分数，而零样本分类方式获得了 0.78 的 F1 分数。如果有标注数据，什么情况下会选择零样本分类？</strong></p><p>即使有标注数据，也会在以下几种情况下选择零样本分类：</p><ol><li>标注数据太少，训练出来的模型效果不稳定，反而不如零样本；</li><li>分类标签经常变，比如新增类别，用零样本不用重新训练；</li><li>需要快速上线，零样本即用即测，不需要模型训练；</li></ol><p><strong>Q14：与 BERT 的掩蔽策略相比，掩码语言建模有何不同？这种预训练方式如何帮助模型在下游的文本分类任务中获得更好的性能？</strong></p><p>BERT 用的就是掩码语言建模（MLM），它会把句子里一部分词换成 <code>[MASK]</code>，然后让模型去猜被遮住的词是什么。这种方法让模型学会理解整个句子的意思，而不是像 GPT 那样只预测下一个词。因为模型在预训练时学会了看一句话的上下文，理解句子的语义，所以在做文本分类时，比如判断情感、话题、意图，只需要在上面接一个简单的分类器，模型就能表现很好。</p><p><strong>Q15：假设你有一个包含 100 万条客户评论的数据集，但只有 1000 条带有标签的数据，请同时利用有标签和无标签的数据，结合表示模型和生成模型的优势，构建一个分类系统。</strong></p><p>Step 1：构建基础表示模型，使用 BERT 类预训练模型，加载 MLM 预训练权重，用这 1000 条标注样本训练</p><p>Step 2：利用无标签数据做伪标签增强，用初始模型对部分高置信度的无标签数据打标签，将这些“伪标签样本”加入训练集，继续训练模型</p><p>Step 3：引入生成模型增强（Prompt + 少样本能力），使用LLM做 zero-shot 或 few-shot 辅助，和 BERT 模型的预测结果进行融合，提升低资源条件下的准确率。</p><p><strong>Q16：有了强大的生成式大模型，嵌入模型还有什么用？</strong></p><ul><li>嵌入模型速度快、成本低，适合做大规模文本检索和相似度计算。</li><li>嵌入能把文本转成向量，方便做分类、聚类和搜索等基础任务。</li><li>生成模型更重，更适合复杂对话和文本生成，嵌入模型更轻量，能做很多高效的辅助工作。</li><li>嵌入模型可以跟生成模型配合使用，提升整体系统效果。</li></ul><p><strong>Q17：词袋法和文档嵌入在实现原理上有什么区别？词袋法是不是一无是处了？</strong></p><p><img src="https://pic3.zhimg.com/v2-a9b2bf8e4acf3dda09d065dbcb54ed2e_1440w.jpg" alt="img"></p><p>词袋法的优势：</p><ul><li><strong>计算快，资源占用低</strong>，适合快速原型和小数据场景；</li><li><strong>可解释性强</strong>，方便理解和调试；</li></ul><p><strong>Q18：<a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=BERTopic&amp;zhida_source=entity" target="_blank" rel="noreferrer">BERTopic</a> 中的 <a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=c-TF-IDF&amp;zhida_source=entity" target="_blank" rel="noreferrer">c-TF-IDF</a> 与传统的 TF-IDF 有何不同？这种差异如何帮助改进主题表示的质量？</strong></p><p>c-TF-IDF 与传统 TF-IDF 的区别：</p><ul><li>传统 TF-IDF 是针对单个文档计算词频和逆文档频率，用来衡量词在文档中的重要性。</li><li>c-TF-IDF 是针对一个“类”或“主题下的所有文档的集合”来计算词频和逆文档频率，强调词在该主题下的重要性，同时抑制在其他主题中的普遍词。</li></ul><p>c-TF-IDF 可以更准确地找到某个主题的<strong>代表性关键词</strong>，因为它把主题内词频和跨主题词频都考虑进去了。这样主题关键词更具区分性，不容易被在所有主题中都出现的高频词干扰。因此，c-TF-IDF 提升了主题的清晰度和区分度，使得主题表示更精准、更有意义。</p><p><strong>Q19：基于质心和基于密度的文本聚类算法有什么优缺点？</strong></p><p>基于质心的文本聚类（如 K-means）优缺点：</p><p>优点：</p><ul><li>算法简单、计算速度快，适合大规模数据；</li><li>易于实现和理解；</li><li>聚类结果通常是凸形，便于解释。</li></ul><p>缺点：</p><ul><li>需要预先指定簇数；</li><li>对初始质心敏感，容易陷入局部最优；</li><li>不能很好处理非凸形簇或噪声数据。</li></ul><p>基于密度的文本聚类（如 DBSCAN）优缺点：</p><p>优点：</p><ul><li>不需要预先指定簇数；</li><li>能发现任意形状的簇；</li><li>能有效识别噪声和离群点。</li></ul><p>缺点：</p><ul><li>对参数（邻域大小、最小点数）敏感，参数选择困难；</li><li>计算复杂度较高，不太适合超大规模数据；</li><li>在高维文本空间中，密度判断可能失效（“维度灾难”）。</li></ul><p><strong>Q20：在一个主题建模项目中，你发现生成的主题中有大量重叠的关键词，如何使用本章介绍的技术来提高主题之间的区分度？</strong></p><p>可以用以下几种技术：</p><ul><li>使用 c-TF-IDF： c-TF-IDF 会针对每个主题集合计算词权重，抑制在多个主题中都高频的词，从而突出每个主题的独特关键词。</li><li>调整主题数量和模型参数：适当调整主题数，避免主题过多导致语义重复；通过正则化或增加主题间距离约束，提高区分度。</li><li>基于密度的聚类方法：采用 DBSCAN 等密度聚类替代 K-means，避免不同主题被划入同一簇。</li><li>关键词后处理：对重复关键词做过滤或降权处理，保留区分度高的词。</li></ul><p><strong>Q21：针对翻译类、创意写作类、头脑风暴类任务，分别如何设置 temperature 和 top_p ？</strong></p><p>任务越标准越严谨（如翻译），<code>temperature</code> 就越低；任务越开放越创新（如写作、头脑风暴），就适当提高 <code>temperature</code> 和 <code>top_p</code>，增强多样性。</p><p><strong>Q22：一个专业的提示词模板由哪几部分构成？为什么提示词中需要描述角色定义？</strong></p><p>一个专业的提示词（Prompt）模板通常包含以下几部分：</p><ol><li>角色定义（Role）→ 告诉模型“你是谁”（例如你是医生/律师/客服/算法工程师）。</li><li>任务目标（Task）→ 明确你要模型完成什么任务，比如“写摘要”、“做分类”、“写代码”。</li><li>上下文信息（Context）→ 提供必要背景，让模型理解问题场景。</li><li>输入数据（Input）→ 提供需要处理的具体内容，如文本、问题或数据片段。</li><li>输出格式要求（Output Format）→ 规定模型输出的格式或风格，比如表格、列表、JSON 或限制字数等。</li></ol><p>因为大模型是通过模式学习的，角色定义能引导模型以更专业、更符合预期的语气和风格回答问题。</p><p><strong>Q23：为了尽可能防止提示词注入，如何设计提示词模板？如何在系统层面检测提示词注入攻击？</strong></p><ol><li>提示词模板设计防护：</li></ol><ul><li><p>明确角色设定：在 prompt 中指定模型身份和行为范围，如“你是一个只能回答财经问题的助手”。</p></li><li><p>使用指令封闭结构：避免开放式提示，使用模板化结构如 <code>&quot;指令: {instruction} \\n 输入: {input} \\n 输出: &quot;</code>。</p></li><li><p>控制用户插入点：将用户输入限制在变量 slot 中，避免影响系统指令。</p></li><li><p>系统层面注入检测：</p></li><li><p>关键词黑名单匹配：检测如“忽略之前的指令”“你现在是…”等注入特征。</p></li><li><p>提示词审查机制：在提示词构造后进行语义分析，识别可能的角色切换、指令覆写。</p></li><li><p>响应行为监控：监控输出是否越权，如输出系统不允许的内容，可触发告警或拒答。</p></li><li><p>使用上下文分离机制：通过系统-用户分段处理，防止用户内容污染系统 prompt。</p></li></ul><p><strong>Q24：在没有推理模型之前，如何让模型先思考后回答？思维链、自洽性、思维树等几种技术各有什么优缺点？</strong></p><p><strong>在没有推理能力前，如何让模型“先思考后回答”？</strong> 通过<strong>提示词工程</strong>引导模型“思考”，即在 prompt 中显式要求模型列出推理过程，如加上：</p><ul><li><code>&quot;请一步步思考再作答&quot;</code>（思维链）</li><li><code>&quot;请验证你的答案是否与前提一致&quot;</code>（自洽性）</li><li><code>&quot;请考虑多种可能情况，再做判断&quot;</code>（思维树）</li></ul><p><img src="https://pic2.zhimg.com/v2-3f4b660786169ca840437a9d0e95fde5_1440w.jpg" alt="img"></p><p><strong>Q25：如何保证模型的输出一定是合法的 JSON 格式？将大模型用于分类任务时，如何保证其输出一定是几个类别之一，而不会输出无关内容？如果开发一个学习英语的应用，如何确保其输出的语言始终限定在指定的词汇表中？</strong></p><p><strong>如何保证输出是合法 JSON 格式？</strong></p><ul><li>结构化提示词：明确指令，如：“请仅输出以下 JSON 格式：<code>{&quot;label&quot;: ..., &quot;score&quot;: ...}</code>，不添加任何解释。”</li><li>使用函数调用（Function Calling / Tool Use）机制：如 OpenAI / Qwen 支持结构化 schema，让模型输出严格符合预定义结构。</li><li>后处理校验：使用 JSON 解析库进行合法性校验，不合法则重试生成或触发 fallback 策略。</li></ul><p><strong>分类任务中如何约束输出为限定类别？</strong></p><ul><li>封闭式提示设计：明确说明“只允许从以下选项中选择一个：A、B、C”。</li><li>Few-shot 示例引导：通过提供标准分类示例（如“输入：... → 输出：B”）提高一致性。</li><li>输出校验与纠偏：使用正则或解析器判断输出是否合法，不合法时触发二次问答或重新生成。</li><li>可选：使用 Logits Bias / 多标签 token sampling 控制输出 token（限 API 层实现）</li></ul><h3 id="英语学习应用中如何限制词汇范围" tabindex="-1"><strong>英语学习应用中如何限制词汇范围？</strong> <a class="header-anchor" href="#英语学习应用中如何限制词汇范围" aria-label="Permalink to &quot;**英语学习应用中如何限制词汇范围？**&quot;">​</a></h3><ul><li>词汇表内提示约束：在提示中加入说明：“请仅使用以下词汇表中的单词作答”，并附上词表。</li><li>使用词表检索后重写（Constrained decoding）：结合规则或重写策略，将非词表内词替换或拒答。</li><li>训练或微调阶段加入词表限制示例：增强模型对“词汇边界”的理解。</li><li>结合语法校正模块二次过滤：检测并剔除越界词汇。</li></ul><p><strong>Q26：如果我们需要生成小说的标题、角色描述和故事梗概，当单次模型调用生成效果不佳时，如何分步生成？</strong></p><p>可以采用<strong>分步生成策略</strong>，分阶段控制质量与连贯性：</p><p>分步流程设计：</p><ul><li>Step 1：生成小说主题或关键词</li><li>Step 2：基于主题生成小说标题</li><li>Step 3：根据标题设定角色</li><li>Step 4：生成故事梗概</li></ul><p>优点：</p><ul><li>更高控制力：每步可加入人工或规则约束，确保输出质量。</li><li>提升上下文一致性：逐步构建世界观，避免信息混乱。</li><li>易于纠错与重试：某一步生成不佳时，仅重试该步。</li></ul><p><strong>Q27：如果用户跟模型对话轮次过多，超出了模型的上下文限制，但我们又希望尽可能保留用户的对话信息，该怎么办？</strong></p><ol><li>信息摘要：对早期对话进行摘要压缩，保留核心内容，如用户目标、偏好、历史提问等。</li><li>结构化记忆：将对话转为结构化格式存入外部 memory，后续构建 prompt 时引用这些结构化内容，而非逐轮堆叠原始对话。</li><li>轮次截断：固定保留最近 N 轮对话，加上摘要或记忆，构成上下文窗口。</li><li>外部记忆+RAG 机制：将历史对话存入向量数据库，用户提问时检索相关历史片段，再拼接进 prompt</li></ol><p><strong>Q28：如何编写一个智能体，帮助用户规划一次包含机票预订、酒店安排和景点游览的旅行？需要配置哪些工具？如何确保系统在面对不完整或矛盾的信息时仍能提供合理建议？</strong></p><p>采用基于工具调用的智能体架构</p><blockquote><p>用户输入 → 智能体分析任务意图 → 规划任务流程 → 多工具调用 → 整合回复结果</p></blockquote><p><img src="https://pic3.zhimg.com/v2-0bd6934418581ff5cf5c232fef6bd5d8_1440w.jpg" alt="img"></p><p>方法：</p><ul><li>利用 LLM 分析用户输入中缺失的关键信息（如日期、预算、偏好），主动进行“反问式追问”</li><li>设定默认值与容错机制，未指定预算则设定默认中档；未指定具体日期，则提示最近可用时间段。</li><li>多轮规划修正能力，使用 ReAct 思维链提示结构，支持思考—行动—观察—再决策循环，自动纠偏</li></ul><p><strong>Q29：如果单一智能体的提示词过长，导致性能下降，如何将其拆分为多个智能体，并在合适的时机调用不同的智能体？</strong></p><p>当单一智能体的提示词过长，影响性能时，我会采用“模块化智能体架构”解决，具体如下：</p><ol><li>功能拆分：将大智能体按功能划分为多个子智能体，例如：行程规划、机票预订、酒店推荐等，每个子体负责单一任务，提示词更短、聚焦。</li><li>主控调度器：设计一个主控Agent作为“路由器”，根据用户请求类型或上下文意图，动态调用对应子体。例如：识别“订酒店”意图后，调用酒店智能体。</li><li>上下文共享：通过共享存储（如Memory模块）或摘要传递，实现子体间信息协同，避免重复问询。</li><li>提示词压缩：对每个子体使用结构化提示+嵌入指令压缩技术，确保上下文长度控制在合理范围内。</li></ol><p>这种方式能提升系统响应效率、增强可维护性，并保持智能体的扩展性。</p><p><strong>Q30：在 RAG 中，为什么要把文档划分成多个块进行索引？如何解决文档分块后内容上下文缺失的问题？如何处理跨片段的依赖关系？</strong></p><p>在 RAG 中，将文档划分为多个块是为了提高召回精度和效率，避免整篇文档过长导致嵌入不准确或超出上下文长度限制。</p><p>为解决分块后的上下文缺失问题，我通常采用以下策略：</p><ol><li>滑动窗口分块：使用一定重叠率的滑窗分块，保留前后句的上下文信息，减少语义割裂。</li><li>块内结构优化：结合标题、段落、位置等结构信息，将语义强相关的内容聚为一块。</li><li>跨片段依赖处理：</li></ol><ul><li>多块召回：允许模型检索多个相邻片段，并在生成阶段融合；</li><li>块间链接：建立块与块之间的逻辑引用，例如使用链式召回或构建知识图谱辅助。</li></ul><p>通过这些方式，RAG 系统能在保持检索效率的同时，尽量还原原始文档的上下文完整性。</p><p><strong>Q31：向量相似度检索不能实现关键词的精确匹配，基于倒排索引的关键词检索不能匹配语义相近的词，如何解决这对矛盾？为什么需要重排序模型？</strong></p><p>为解决语义检索与关键词匹配的矛盾，我采用混合检索架构：</p><ol><li>向量检索召回语义相近文本，解决同义词、表达差异问题；</li><li>关键词检索通过倒排索引确保精确命中重要术语；</li><li>融合策略：两种方式结合，互补召回，提升覆盖率与准确性。</li></ol><p>重排序模型的作用是对初步召回结果进行语义理解与排序优化，解决召回阶段排序不准的问题。它通常使用跨编码器或大模型，根据查询与文档间深层语义关系重新打分，提升最终输出质量。</p><p><strong>Q32：为什么要在向量相似度检索前，对用户输入的话进行改写？</strong></p><p>在向量相似度检索前对用户输入进行改写，目的是提升检索效果，具体原因如下：</p><ol><li>语义增强：用户查询可能简短或模糊，改写可以补全上下文，表达更清晰的语义意图。</li><li>与文档风格对齐：用户用语和文档表述方式可能不同，改写有助于统一语言风格，提升嵌入相似度。</li><li>避免歧义：消除查询中的多义词或口语表达，使向量更准确地表达用户真实意图。</li><li>引导模型聚焦关键点：通过重构问题，引导嵌入模型关注更相关的信息，提升召回精准度。</li></ol><p><strong>Q33：如果需要根据某长篇小说的内容回答问题，而小说的长度远远超出了上下文限制，应该如何综合利用摘要和 RAG 技术，使其能同时回答故事梗概和故事细节？</strong></p><p>面对长篇小说超过上下文限制的问题，我会综合使用摘要机制与RAG 技术，具体方案如下：</p><ol><li>全局摘要生成：先对整本小说生成一份结构化摘要（如人物关系、章节梗概、主线剧情），用于回答“故事梗概”类问题。</li><li>文档分块 + RAG 检索：将小说按章节或段落分块，构建向量索引，结合语义检索用于“细节问题”回答，如某人物对白、具体事件等。</li><li>融合式回答策略：</li></ol><ul><li>问题若偏向全局，直接使用摘要内容作答；</li><li>问题若涉及细节，触发 RAG 检索相关片段；</li><li>如有需要，摘要可作为补充上下文与检索内容一并输入模型，增强回答连贯性。</li></ul><p><strong>Q34：在 <a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=CLIP&amp;zhida_source=entity" target="_blank" rel="noreferrer">CLIP</a> 训练过程中，为什么需要同时最大化匹配图文对的相似度和最小化非匹配图文对的相似度？</strong></p><p>在 CLIP 训练过程中，同时最大化匹配图文对的相似度、最小化非匹配对的相似度，是为了实现多模态对齐和区分，核心目的有两点：</p><ol><li>对齐语义空间：最大化真实图文对相似度，让图像和文本嵌入映射到统一语义空间，确保语义相关的图文能相互识别。</li><li>增强判别能力：最小化非匹配对的相似度，拉开正负样本距离，防止模型仅学习“图像或文本的单模态特征”，提升检索和匹配的准确性。</li></ol><p>这种对比学习目标确保模型既能“找到正确的对”，又能“区分错误的对”，从而具备强泛化能力与多模态理解能力。</p><p><strong>Q35：<a href="https://zhida.zhihu.com/search?content_id=259688508&amp;content_type=Article&amp;match_order=1&amp;q=BLIP-2&amp;zhida_source=entity" target="_blank" rel="noreferrer">BLIP-2</a> 为何不直接将视觉编码器的输出连接到语言模型，而要引入 Q-Former 这一中间层结构</strong></p><p>BLIP-2 引入 <strong>Q-Former</strong> 而不是直接将视觉编码器输出接入语言模型，主要有三个原因：</p><ol><li><strong>桥接模态差异</strong>：视觉编码器输出是高维 dense 的 patch embedding，不适合直接输入语言模型。Q-Former 通过少量 learnable queries 提取压缩后的语义信息，起到模态对齐的桥梁作用。</li><li><strong>降低输入冗余</strong>：直接传入所有图像特征会超长且噪声多，Q-Former 用注意力机制从视觉特征中提取关键信息，减少 token 数，提高效率和效果。</li><li><strong>增强信息交互</strong>：Q-Former 是 transformer 架构，可以在视觉上下文中进行交叉注意力，自适应地选择哪些视觉信息对当前任务更重要。</li></ol><p><strong>Q36：现有一个能力较弱的多模态模型和一个能力较强的文本模型（如 DeepSeek-R1），如何结合两者的能力来回答与多模态相关的问题？</strong></p><p>可以通过<strong>两阶段协同框架</strong>结合弱多模态模型与强文本模型，方式如下：</p><p>阶段一：多模态理解（由弱多模态模型完成） 使用弱多模态模型对图像+文本输入进行理解，输出中间表示，如：</p><ul><li><ul><li>图像描述（caption）</li><li>物体/关系识别结果</li><li>图文对齐的结构化摘要或关键信息片段</li></ul></li></ul><p>阶段二：语言生成与推理（交给强文本模型处理） 将阶段一输出连同原始问题一并输入强大的文本模型（如 DeepSeek-R1），由其执行复杂推理、问答、对话生成等任务。</p><p>弱多模态模型负责提取视觉语义信息，强语言模型负责推理与表达，实现“感知-推理分离”，充分发挥两者优势。</p><p><strong>Q37：如何构建一个 AI 照片助手，能够对用户的上万张照片进行索引，根据用户的查询高效地检索相关照片？</strong></p><p>构建 AI 照片助手的简要方案：</p><ol><li><strong>照片预处理</strong>：对所有照片进行去重、清晰度检测和格式统一。</li><li><strong>特征提取</strong>：使用预训练的视觉模型（如ResNet、CLIP）提取每张照片的视觉特征向量。</li><li><strong>索引构建</strong>：利用高效的向量数据库（如FAISS、Milvus）建立照片特征的索引，支持快速相似度搜索。</li><li><strong>查询解析</strong>：将用户查询转换为语义向量（文本转向量），如使用CLIP文本编码器或其他文本嵌入模型。</li><li><strong>相似度检索</strong>：在索引中用查询向量检索最相似的照片特征，返回相关照片。</li><li><strong>排序与过滤</strong>：根据时间、地点、标签等元数据对结果进行二次排序和筛选。</li><li><strong>用户反馈</strong>：允许用户标记相关或不相关照片，用于后续模型微调和提升检索效果。</li></ol><p><strong>Q38：相比交叉编码器，为什么双编码器在大规模相似度搜索中更受欢迎？</strong></p><p>双编码器更受欢迎因为：</p><ol><li><strong>效率高</strong>：双编码器可离线预计算所有数据向量，在线只需计算查询向量并快速检索。</li><li><strong>检索速度快</strong>：支持使用向量索引库（如FAISS）实现亚秒级近似搜索。</li><li><strong>扩展性强</strong>：适合海量数据，计算成本低，易于分布式部署。</li></ol><p>交叉编码器需每次对查询和候选项一起编码，计算量大，无法快速扩展。</p><p><strong>Q39：在训练嵌入模型时，MNR（多负例排序）损失、余弦相似度损失和 softmax 损失各有哪些优缺点？在哪些场景下，余弦相似度损失可能比 MNR 损失更合适？</strong></p><ul><li><p>MNR（多负例排序）损失</p></li><li><ul><li>优点：利用多个负样本，提升判别力；效果好，收敛快。</li><li>缺点：计算复杂，负样本采样质量影响大。</li></ul></li><li><p>余弦相似度损失</p></li><li><ul><li>优点：直接优化向量夹角，相对简单，适合向量归一化场景。</li><li>缺点：对负样本区分能力较弱，可能导致泛化不足。</li></ul></li><li><p>Softmax损失</p></li><li><ul><li>优点：可视为分类，训练稳定，容易优化。</li><li>缺点：负样本一般固定且有限，难以扩展到大规模负样本。</li></ul></li></ul><p>余弦相似度损失更合适的场景： 当负样本难以采集或训练资源有限，且模型需要稳定向量空间结构，尤其是在小样本、在线微调或向量归一化强制约束时，用余弦相似度损失更简单高效。</p><p><strong>Q40：如何生成负例以提升模型性能？如何构建高质量的难负例？</strong></p><p>生成负例提升性能的方法：</p><ul><li>随机负采样：随机选取非匹配样本，简单但效果有限。</li><li>硬负采样（Hard Negative Mining）：选择与正样本相似度较高但不匹配的样本，能显著提升模型判别能力。</li><li>动态负采样：训练过程中不断更新负例集合，使负例更贴近当前模型难区分样本。</li></ul><p>构建高质量难负例的方法：</p><ul><li>利用现有模型检索出与正样本最相似的错误匹配项。</li><li>通过领域知识筛选容易混淆的样本。</li><li>利用对抗生成方法制造与正样本高度相似但标签不同的样本。</li></ul><p><strong>Q41：为什么 TSDAE 选择使用特殊词元而非平均池化作为句子表征？</strong></p><p>TSDAE 用特殊词元（如 [CLS]）作为句子表征，因为：</p><ol><li>聚合效果好：特殊词元在训练中被优化为捕捉整句信息，表征更集中和语义丰富。</li><li>计算效率高：只需取一个向量，无需额外计算平均池化所有词向量。</li><li>避免噪声：平均池化可能被无关词或停用词干扰，特殊词元更稳健。</li></ol><p><strong>Q42：相比 STSB，MTEB 有哪些改进？其中包括哪些类别的嵌入任务？</strong></p><p>相比 STSB，MTEB（Massive Text Embedding Benchmark）改进在于：</p><ol><li>任务更多样：覆盖更广泛的嵌入任务类型，不仅限于语义相似度。</li><li>数据规模更大：包含更多数据集，评测更全面。</li><li>细分任务类别：支持检索、分类、聚类等多种应用场景。</li><li>统一评测框架：便于多模型、多任务的系统比较。</li></ol><p>MTEB涵盖的嵌入任务类别主要包括：</p><ul><li>语义文本相似度（STS）</li><li>文本检索（Information Retrieval）</li><li>文本分类（Classification）</li><li>聚类（Clustering）</li><li>召回（Ranking）</li><li>句子对判别（Sentence Pair Tasks）</li><li>其他多模态或跨语言任务</li></ul><p><strong>Q43：如果标注的训练数据很少，如何扩增训练数据的数量？（提示：SetFit）</strong></p><p>当标注数据少时，可以用 SetFit 方法扩增数据：</p><ul><li>利用少量标注样本，先用小型句子嵌入模型生成文本向量。</li><li>通过聚类或最近邻检索自动挑选未标注数据中与标注样本相似的文本。</li><li>对这些相似文本自动赋予伪标签，形成扩增训练集。</li><li>用扩增数据微调简单的分类器（如基于句向量的轻量模型）。</li></ul><p><strong>Q44：在继续预训练时，如何在保证模型获得特定领域知识的同时，最大限度地保留其通用能力？</strong></p><p>继续预训练时，为兼顾特定领域知识和通用能力，可以：</p><ol><li>混合数据训练：将通用语料和领域语料按一定比例混合训练，防止模型过拟合领域数据。</li><li>小步长微调：使用较小学习率，避免模型参数剧烈偏移。</li><li>正则化方法：如知识蒸馏、L2正则限制参数变化，保护原有能力。</li><li>多任务训练：同时训练通用任务和领域任务，保持广泛泛化。</li><li>参数冻结：部分层冻结只微调特定层，减少通用知识丢失。</li></ol><p><strong>Q45：请比较以下三种方案在医疗领域文本分类任务上的优缺点：(a) 直接使用通用 BERT 模型微调； (b) 在医疗文本上继续预训练 BERT 后再微调；(c) 从头开始用医疗文本预训练模型再微调。</strong></p><p>(a) 直接用通用 BERT 微调</p><ul><li>优点：训练成本低，实施快。</li><li>缺点：领域适应差，效果可能受限，特别是专业术语理解不足。</li></ul><p>(b) 医疗文本上继续预训练后微调</p><ul><li>优点：融合通用知识和领域特征，效果较好，成本适中。</li><li>缺点：需要额外预训练资源，参数可能过拟合领域数据。</li></ul><p>(c) 从头开始用医疗文本预训练再微调</p><ul><li>优点：模型完全适应领域，表现潜力最大。</li><li>缺点：计算成本极高，需大量高质量医疗数据，训练周期长。</li></ul><p>总结： (b)方案通常在效果和成本间取得最佳平衡，是医疗领域主流选择。</p><p><strong>Q46：在命名实体识别任务中，当 BERT 将单词拆分成多个词元时，如何解决标签对齐问题？</strong></p><p>当 BERT 将单词拆成多个词元时，标签对齐常用方法有：</p><ol><li>只给第一个词元贴标签，后续词元用特殊标签（如 <code>X</code>）或忽略，不计算损失。</li><li>第一个词元标签复制到所有拆分词元，保持一致标签。</li><li>使用子词级别标签体系，对每个词元单独标注，但复杂且不常用。</li></ol><p><strong>Q47：假设一个嵌入模型的训练语料主要由英文构成，在中文任务上表现不佳，如何用较低的继续预训练成本提升其中文能力？</strong></p><p>用低成本提升英文主导嵌入模型的中文能力，可以：</p><ol><li>使用中文文本做领域小规模继续预训练，学习中文特征。</li><li>混合中英文数据训练，防止通用能力丢失。</li><li>利用多语言词表或添加中文词汇扩展词表。</li><li>使用跨语言对齐任务（如翻译对、对比学习）增强中文语义。</li><li>微调时重点加强中文下游任务，配合数据增强。</li></ol><p>这样在有限资源下，有效提升模型中文表现。</p><p><strong>Q48：有人声称一篇文章是用 DeepSeek-R1 生成的，并给了你生成所用的完整提示词，如何证实或证伪这个说法？（提示：利用困惑度）</strong></p><p>验证文章是否由 DeepSeek-R1 生成，可以：</p><ol><li>用 DeepSeek-R1 模型和给定提示词，对文章文本计算<strong>困惑度（Perplexity）</strong>。</li><li>生成文本困惑度显著低，说明模型更可能生成该文。</li><li>如果困惑度高，则文章不太可能是该模型用该提示词生成。</li></ol><p>困惑度是衡量模型对文本“熟悉度”的指标，低困惑度支持生成假设，高困惑度则反驳。</p><p><strong>Q49：如何微调一个 Llama 开源模型，使其输出风格更简洁、更像微信聊天，并保证输出的内容符合国内的大模型安全要求？</strong></p><p>微调 Llama 模型实现微信聊天风格且符合安全要求，可以：</p><ol><li>数据准备：收集大量微信聊天风格的对话数据，涵盖简洁、口语化表达。</li><li>安全数据过滤：剔除敏感、违规内容，确保训练数据符合国内法规和平台安全规范。</li><li>微调策略：用低学习率微调模型，强化简洁对话风格，避免过拟合。</li><li>安全对齐：结合规则过滤或安全检测模块，训练时加入安全指令或使用 RLHF（强化学习从人类反馈）强化合规输出。</li><li>持续评估：通过人工审核和自动检测工具，确保输出既简洁又合规。</li></ol><p><strong>Q50：QLoRA 中的分块量化如何解决普通量化导致的信息损失问题？</strong></p><p>QLoRA 的分块量化通过将模型权重按小块（block）独立量化，解决普通量化信息损失问题，具体体现在：</p><ol><li>局部自适应量化：每个小块单独计算缩放因子，保留更多局部细节和动态范围。</li><li>减少量化误差累积：避免整块或全局量化带来的误差集中，提升表达精度。</li><li>兼顾压缩率与精度：在保持高压缩的同时，有效保留关键权重信息。</li></ol><p><strong>Q51：现有一个由若干篇文章组成的企业知识库，如何将其转换成适合 SFT 的数据集？</strong></p><p>将企业知识库转成适合SFT的数据集，步骤如下：</p><ol><li>拆分文章：将长文拆成合理长度的段落或问答对。</li><li>构造输入-输出对：输入为问题、上下文或摘要，输出为对应的答案或摘要。</li><li>规范格式：转换为模型支持的格式，如 JSONL，包含字段 <code>prompt</code> 和 <code>response</code>。</li><li>清洗数据：去除无关或重复内容，确保质量。</li><li>扩充多样性：可人工或自动生成多种问法，增强泛化能力。</li></ol><p><strong>Q52：PPO 和 DPO 相比有什么优缺点？</strong></p><p>PPO（Proximal Policy Optimization）</p><ul><li>优点：强化学习方法，能直接优化模型行为；稳定且效果好；支持复杂策略更新。</li><li>缺点：训练复杂，调参难度大；需要环境反馈和奖励信号。</li></ul><p>DPO（Direct Preference Optimization）</p><ul><li>优点：无需环境交互，基于偏好数据直接优化；训练更简单、效率更高；适合离线数据。</li><li>缺点：依赖高质量偏好标签；难处理多样复杂策略。</li></ul><p>总结： PPO 灵活但训练成本高，适合在线交互；DPO 简单高效，更适合离线带偏好数据的优化。</p><p><strong>Q53：在 PPO 中，如何防止模型在微调数据集以外的问题上泛化能力下降？如何防止模型收敛到单一类型的高奖励回答？</strong></p><p>防止泛化能力下降：</p><ul><li>在微调时加入原始预训练数据或多样化样本，进行混合训练。</li><li>使用KL散度约束限制新策略与原始模型差异，防止偏离太远。</li><li>适度正则化，保持模型通用性。</li></ul><p>防止收敛到单一高奖励回答：</p><ul><li>引入奖励多样性机制，鼓励回答多样化。</li><li>使用熵正则化增加策略探索，避免过早收敛。</li><li>设计多元化奖励函数，平衡准确性与多样性。</li></ul><p><strong>Q54：设想一个网站上都是 AI 生成的内容，我们统计了每篇文章的平均用户停留时长，如何将其转化为 DPO 所需的偏好数据？对于小红书和知乎两种类型的网站，处理方式有什么区别？</strong></p><p>转化方法：</p><ul><li>利用平均停留时长作为用户偏好的间接信号，停留时间长的文章优先级高。</li><li>构建文章对（A、B），若A停留时长明显高于B，则偏好标签为“用户更喜欢A”。</li><li>生成偏好对数据，作为DPO训练的输入。</li></ul><p>小红书 vs 知乎区别：</p><ul><li>小红书内容偏视觉和生活方式，停留时间可能受图片、短视频影响，需结合多模态信号和互动数据（点赞、收藏）辅助判断。</li><li>知乎偏文字和深度问答，停留时长更直接反映内容质量，偏好构建可更直接依赖停留时间和评论质量。</li></ul><p><strong>Q55：提示工程、RAG、SFT、RL、RLHF 应该分别在什么场景下应用？例如：快速迭代基本能力（提示工程）、用户个性化记忆（提示工程）、案例库和事实知识（RAG）、输出格式和语言风格 （SFT）、领域深度思考能力和工具调用能力（RL）、根据用户反馈持续优化（RLHF）</strong></p><p>提示工程（Prompt Engineering）</p><ul><li>快速迭代和调试模型基本能力</li><li>实现简单用户个性化和上下文记忆</li><li>无需训练，低成本实验方案</li></ul><p>RAG（Retrieval-Augmented Generation）</p><ul><li>利用外部案例库和事实知识补充模型知识盲点</li><li>实时访问动态或专业知识库</li><li>提高生成内容的准确性和时效性</li></ul><p>SFT（Supervised Fine-Tuning）</p><ul><li>调整输出格式、语言风格和固定任务表现</li><li>融合专业领域或企业特定风格</li><li>提升模型对特定指令的响应能力</li></ul><p>RL（Reinforcement Learning）</p><ul><li>培养模型的深度思考、策略选择和工具调用能力</li><li>优化复杂任务的长期回报表现</li><li>适合交互式和多步骤决策场景</li></ul><p>RLHF（Reinforcement Learning from Human Feedback）</p><ul><li>根据用户或专家反馈持续优化模型输出质量</li><li>处理主观偏好、多样性和安全性问题</li><li>提升模型对人类意图的精准理解与执行</li></ul><p><strong>Q56：DeepSeek-R1 （简称 R1）与 DeepSeek-R1-Zero（简称 R1-Zero）的训练过程有什么区别，各自有什么优缺点？既然 R1-Zero 生成的推理过程可读性差，在非推理任务上的表现也不如 R1，那么 R1-Zero 存在的价值是什么？ R1 训练过程是如何解决 R1-Zero 的上述问题的？</strong></p><p>训练过程区别：</p><ul><li>R1-Zero：仅用少量零样本（zero-shot）数据训练，依赖模型自身推理能力，无强化人类反馈。</li><li>R1：基于R1-Zero，加入了有监督微调（SFT）和强化学习（如RLHF），结合人类反馈优化推理过程和答案质量。</li></ul><p>优缺点：</p><ul><li><p>R1-Zero</p></li><li><ul><li>优点：训练成本低，快速部署。</li><li>缺点：推理过程不透明，生成结果不够准确且不稳定。</li></ul></li><li><p>R1</p></li><li><ul><li>优点：推理过程清晰可读，回答准确且符合人类期望。</li><li>缺点：训练成本较高，需大量标注和反馈。</li></ul></li></ul><p>R1-Zero的价值：</p><ul><li>作为快速原型和基线，验证模型零样本能力。</li><li>为后续SFT和RLHF提供初始化权重。</li><li>适合对推理可解释性要求不高的场景。</li></ul><p>R1如何解决问题：</p><ul><li>通过SFT让模型学习规范化推理表达。</li><li>用RLHF优化模型生成的推理路径和答案质量，使推理过程连贯且可信。</li><li>结合人类反馈纠正错误，提升稳定性和准确性。</li></ul><p><strong>Q57：DeepSeek 是如何把 R1 的推理能力蒸馏到较小的模型中的？如果我们要自己蒸馏一个较小的垂直领域模型，如何尽可能保留 R1 在特定领域的能力？</strong></p><p>DeepSeek蒸馏R1推理能力的方法：</p><ul><li>利用大模型R1生成带有推理过程的高质量“教师”示例（含step-by-step解答）。</li><li>小模型通过监督学习模仿教师示例，不仅学习最终答案，还学习推理步骤。</li><li>结合专门设计的蒸馏损失（如过程一致性损失），强化推理路径的迁移。</li><li>适当剪裁和微调，保持推理连贯与准确。</li></ul><p>自己蒸馏垂直领域模型保留R1能力建议：</p><ol><li>采集高质量领域内推理示例，用R1生成并人工筛选。</li><li>设计多任务蒸馏目标，同时优化答案和推理过程匹配。</li><li>加强领域知识注入，在蒸馏数据中包含丰富领域术语和知识点。</li><li>采用阶段性训练：先蒸馏基础能力，再针对领域微调。</li><li>利用知识蒸馏与任务蒸馏结合，提高小模型泛化与推理能力。</li></ol><p><strong>Q58：R1-Zero 的方法主要适用于有明确验证机制的任务（如数学、编程），如何将这一方法扩展到更主观的领域（如创意写作或战略分析）？</strong></p><p>将 R1-Zero 方法扩展到主观领域，可采用：</p><ol><li>引入多样化评估指标：结合内容创新性、逻辑合理性、情感表达等软指标，设计自动或半自动评价机制。</li><li>利用人类反馈强化训练：结合 RLHF，采集专家或用户对生成内容的偏好反馈。</li><li>多样化训练样本：用丰富多元的创意或战略案例扩充训练集，提升模型创造力和灵活性。</li><li>设计开放式推理模板：鼓励模型生成多路径、多角度的推理过程，增强内容深度与多样性。</li><li>混合引导策略：结合提示工程与零样本推理，灵活引导模型生成符合主观需求的内容。</li></ol><p><strong>Q59：如果要在一个非推理型模型的基础上通过强化学习（RL）训练出一个 1000 以内的整数四则运算错误率低于 1% 的模型，预计基座模型至少需要多大？ RL 过程需要多少张 GPU 和多少训练时长？ （提示：TinyZero）</strong></p><p>基座模型建议在7B模型以上可平衡效果与成本，满足1000以内四则运算的复杂度需求。</p><p>GPU需求：</p><ul><li>训练阶段：参考TinyZero在veRL框架下的配置，需2张A100 80G（或等效算力如H100）。1张卡处理环境模拟（表达式生成与验证），1张卡运行策略模型更新</li><li>分布式扩展：若需加速训练或支持更大batch size，可扩展至4张A100（需高带宽网络避免通信瓶颈）</li></ul><p>训练时长：</p><ul><li><p>关键因素：基座模型初始数学能力、奖励函数设计复杂度。</p></li><li><p>预估范围：</p></li><li><ul><li>基座已具备基础运算能力（如50%准确率）：12-24小时（约5万步）。</li><li>基座能力较弱（如随机输出）：3-5天</li></ul></li></ul><p><strong>Q60：在 QwQ-32B 推理模型的基础上，通过 RL 在类似 OpenAI Deep Research 的场景中强化垂直领域能力，如何构建训练数据集？预计需要多少张 GPU 和多少训练时长？</strong></p><p>训练数据集构建，领域知识库：学术论文、代码库、行业文档、书籍构建包含专家标注的问答对、对话或任务执行数据；使用强模型构建合成数据集或者加入噪音样本，提高数据集多样性。</p><p>GPU需求</p><ul><li><p>基础配置：8×A100 80G（或等效H100），分配如下：</p></li><li><ul><li>4卡：环境模拟（代码执行/工具调用）</li><li>2卡：策略模型推理</li><li>2卡：梯度更新与奖励计算</li></ul></li></ul><p>训练时长</p><ul><li><p>冷启动阶段（基座适配）：</p></li><li><ul><li>20小时：监督微调（SFT）注入领域数据，学习率2e-5，批量大小64</li></ul></li><li><p>RL强化阶段：</p></li><li><ul><li>数学/编程任务：48小时（10万样本，PPO算法，奖励基于代码执行正确性）</li><li>通用科研能力：24小时（5万样本，规则奖励+人工反馈RLHF）</li></ul></li><li><p>总时长：≤4天</p></li></ul><p>到这里我们的面试小结就写完了，因为答主的个人能力限制，可能有些地方写的不完善甚至又错误，欢迎大家匹配指正。</p>',256)])])}const d=i(r,[["render",e]]);export{m as __pageData,d as default};
