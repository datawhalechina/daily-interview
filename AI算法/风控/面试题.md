# 风控算法的一些面试题小结

> 来源：https://zhuanlan.zhihu.com/p/719621121

## 1.**[逻辑回归](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=逻辑回归&zhida_source=entity)的优缺点，局限性在哪？**

优点：

- 实现简单，速度快，占用内存小，可在短时间内迭代多个版本的模型。
- 模型的可解释性非常好，可以直接看到各个特征对模型结果的影响，可解释性在金融领域非常重要，所以在目前业界大部分使用的仍是[逻辑回归模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=逻辑回归模型&zhida_source=entity)。
- 模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，[鲁棒性](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=鲁棒性&zhida_source=entity)更强。
- 特征工程做得好，模型的效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。
- 模型的结果可以很方便的转化为[策略规则](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=策略规则&zhida_source=entity)，且线上部署简单。

2）缺点和局限性:

- 容易欠拟合，相比[集成模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=集成模型&zhida_source=entity)，准确度不是很高。
- 对数据的要求比较高，逻辑回归对缺失值，异常值，[共线性](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=共线性&zhida_source=entity)都比较敏感，且不能直接处理非线性的特征。所以在数据清洗和特征工程上会花去很大部分的时间。
- 在金融领域对场景的适应能力有局限性，例如数据不平衡问题，[高维特征](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=高维特征&zhida_source=entity)，大量多类特征，逻辑回归在这方面不如[决策树](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=决策树&zhida_source=entity)适应能力强。

总结而言 就是 模型简单、解释性强、方便部署、对负责数据处理能力有限。

## 2.逻辑回归输出值是0到1之间的值，这个值是真实的概率吗？

逻辑回归输出的值在0到1之间，并非所有在0到1之间的值都可以表示概率。这个0到1之间的值是通过[sigmoid函数](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=sigmoid函数&zhida_source=entity)将线性回归的结果映射到0到1之间，我们也可以采用其他函数将数值映射到非0到1之间，因此，用sigmoid函数得到的结果并不是绝对的真实概率值，但是可以认为他是一个接近真实概率的值。

## 3.逻辑回归做分类时样本应满足什么条件

逻辑回归做分类时样本应满足的条件包括因变量为分类变量、观测间独立性、样本量要求、线性关系、无[多重共线性](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=多重共线性&zhida_source=entity)、无明显的离群点等。

## 4.**逻辑回归解决过拟合的方法有哪些？**

- 减少特征数量，在实际使用中会用很多方法进行特征筛选，例如基于[IV值](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=IV值&zhida_source=entity)的大小，变量的稳定性，变量之间的相关性等。
- [正则化](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=正则化&zhida_source=entity)，常用的有L1正则化和L2正则化。

## 5.**什么是特征的离散化和特征交叉？逻辑回归为什么要对特征进行离散化？**

- [特征离散化](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=特征离散化&zhida_source=entity)是将数值型特征（一般是连续型的）转变为离散特征，例如评分卡中的woe转化，就是将特征进行分箱，再将每个分箱映射到[woe值](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=woe值&zhida_source=entity)上，就转换为了离散特征。[特征交叉](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=2&q=特征交叉&zhida_source=entity)也叫作特征组合，是将单独的特征进行组合，使用相乘/相除/[笛卡尔积](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=笛卡尔积&zhida_source=entity)等形成合成特征，有助于表示非线性关系。比如使用One-Hot向量的方式进行特征交叉。这种方式一般适用于离散的情况，我们可以把它看做基于业务理解的逻辑和操作，例如经度和纬度的交叉，年龄和性别的交叉等。

- 实际工作中很少直接将连续型变量带入逻辑回归模型中，而是将特征进行离散化后再加入模型，例如评分卡的分箱和woe转化。这样做的优势有以下几个：

- - 1）特征离散化之后，起到了简化模型的作用，使模型变得更稳定，降低了模型过拟合的风险。
  - 2）离散化之后的特征对异常数据有很强的鲁棒性，实际工作中的哪些很难解释的异常数据一般不会做删除处理，如果特征不做离散化，这个异常数据带入模型，会给模型带来很大的干扰。
  - 3）离散特征的增加和减少都很容易，且稀疏向量的[内积乘法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=内积乘法&zhida_source=entity)运算速度快，易于模型的快速迭代。
  - 4）逻辑回归属于[广义线性模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=广义线性模型&zhida_source=entity)，表达能力有限，特征离散化之后，每个离散变量都有单独的权重，相当于给模型引入了非线性，能够提高模型的表达能力。
  - 5）离散化后的特征可进行特征交叉，进一步引入非线性，提高模型的表达能力。

总而言之，特征离散化就是将数值型特征转为离散型特征，便于逻辑回归进行建模，特征交叉就是提高特征的非线性关系。

**6.做评分卡中为什么要进行WOE化？**

- 更好的解释性，变量离散化之后可将每个箱体映射到woe值，而不是通常做one-hot转换。
- woe化之后可以计算每个变量的IV值，可用来筛选变量。
- 对[离散型变量](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=离散型变量&zhida_source=entity)，woe可以观察各个level间的跳转对odds的提升是否呈线性。
- 对连续型变量，[woe](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=7&q=woe&zhida_source=entity)和IV值为分箱的合理性提供了一定的依据，也可分析变量在业务上的可解释性。
- 用[woe编码](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=woe编码&zhida_source=entity)可以处理缺失值问题。

## 7.**逻辑回归的[特征系数](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=特征系数&zhida_source=entity)的绝对值可以认为是特征的重要性吗？**

首先特征系数的绝对值越大，对分类效果的影响越显著，但不能表示系数更大的特征重要性更高。因为改变变量的尺度就会改变系数的绝对值，而且如果特征是线性相关的，则系数可以从一个特征转移到另一个特征，特征间相关性越高，用系数解释变量的重要性就越不可靠。

## 8.逻辑回归为什么要用[极大似然函数](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=极大似然函数&zhida_source=entity)作为损失函数？

![img](https://picx.zhimg.com/v2-92b4c97fbc3cf76a9b14d839ca4a247f_1440w.jpg)



## 9.**决策树模型的优缺点及适用性？**

优点

- 直观易理解：决策树的结构类似于人类的决策过程，因此它的机制解释起来简单，即使是非专业人士也能理解其工作原理。
- 可解释性强：决策树可以生成清晰的规则，这些规则可以直接用于业务决策或者进一步的分析。
- 数据处理能力：决策树能够处理标称型和数值型数据，同时也能处理有缺失属性的样本以及不相关的特征。
- 运行速度快：在测试数据集时，决策树的运行速度比较快，这使得它在实际应用中非常高效。
- 适应性强：[决策树算法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=决策树算法&zhida_source=entity)适用于小数据集，并且[时间复杂度](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=时间复杂度&zhida_source=entity)较小，适合快速处理大量数据。

缺点

- 容易过拟合：[决策树模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=2&q=决策树模型&zhida_source=entity)容易对训练数据过度学习，导致泛化性能差，即在新数据上的表现可能不佳。
- 稳定性问题：决策树对于数据的微小变化可能非常敏感，这可能导致模型的不稳定。
- 复杂性控制：随着数据量的增加，决策树可能会变得过于复杂，难以管理和解释。

## 10.**简述一下决策树的原理以及树的构建过程。**

决策树时基于树的结构进行决策的，学习过程包括[特征选择](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=特征选择&zhida_source=entity)，决策树的生成和剪枝过程。决策树的学习过程通常是递归地选择最优特征，并用最优特征对数据集进行分割。决策树通过一系列的分裂和判断条件来预测目标变量的值，其构建过程是一个递归地将数据集划分为越来越小的子集的过程，直到满足停止条件。

## **11.简述一下ID3，C4.5，CART三类决策树的原理和异同点。**

- ID3选择最佳分割点是基于[信息增益](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=信息增益&zhida_source=entity)的，信息增益越大，表明使用这个属性来划分所获得的“纯度提升”越大。C4.5对ID3进行了改进，因为ID3使用的信息增益对数据划分时，可能出现每个结点只包含一个样本，这些子节点的纯度已经达到最大，但是，这样的决策树并不具有[泛化能力](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=泛化能力&zhida_source=entity)，无法对新样本进行预测。且ID3不能处理连续型变量和缺失值。而C4.5使用[信息增益率](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=信息增益率&zhida_source=entity)来选择属性，克服了信息增益选择属性时偏向选择值多的属性的不足。且可以处理连续型变量和缺失值。
- C4.5是基于ID3的改进版，只能用于分类。而CART树既可以做分类，也可以做回归。CART的本质是对特征空间进行二元划分，所以CART生成的是一颗二叉树，且可以对类别型变量和数值型变量进行分裂。对分类型变量进行划分时，分为等于该属性和不等于该属性，在对连续型变量进行划分时，分为大于和小于，在做分类是使用的是GINI系数作为划分标准，在做回归时使用的是[均方误差](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=均方误差&zhida_source=entity)。

## 12.**决策树对缺失值是如何处理的？**

决策树处理缺失要考虑以下三个问题：

当开始选择哪个属性来划分数据集时，样本在某几个属性上有缺失怎么处理：

- 忽略这些缺失的样本。
- 填充缺失值，例如给属性A填充一个均值或者用其他方法将缺失值补全。
- 计算信息增益率时根据缺失率的大小对信息增益率进行打折，例如计算属性A的信息增益率，若属性A的缺失率为0.9，则将信息增益率乘以0.9作为最终的信息增益率。

一个属性已经被选择，那么在决定分割点时，有些样本在这个属性上有缺失怎么处理？

- 忽略这些缺失的样本。
- 填充缺失值，例如填充一个均值或者用其他方法将缺失值补全。
- 把缺失的样本，按照无缺失的样本被划分的子集样本个数的相对比率，分配到各个子集上去，至于那些缺失样本分到子集1，哪些样本分配到子集2，这个没有一定准则，可以随机而动。
- 把缺失的样本分配给所有的子集，也就是每个子集都有缺失的样本。
- 单独将缺失的样本归为一个分支。

决策树模型构建好后，测试集上的某些属性是缺失的，这些属性该怎么处理？

- 如果有单独的缺失值分支，依据此分支。
- 把待分类的样本的属性A分配一个最常出现的值，然后进行[分支预测](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=分支预测&zhida_source=entity)。
- 待分类的样本在到达属性A结点时就终止分类，然后根据此时A结点所覆盖的叶子节点类别状况为其分配一个发生概率最高的类。

总结而言就是 删除、填充、分配权重、专门分支、概率最大类别

## **13.为什么决策树不需要对数据做归一化等预处理？**

决策树是一种[概率模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=概率模型&zhida_source=entity)，所以不需要做归一化，因为它不关心变量的值，而是关心变量的分布和变量之间的条件概率，所以归一化这种数值缩放，不影响分裂结点位置。

## 14.**如何解决决策树的过拟合问题？**

剪枝技术

- 预剪枝：预剪枝是在决策树完全生成之前就停止树的构建过程。这种方法可以通过设定树的最大深度、节点包含的最小样本数等规则来实现。预剪枝的优点是简单易行，但缺点是很难精确控制何时停止增长，可能会导致欠拟合。
- 后剪枝：后剪枝是先让决策树完整生成，然后根据一定的标准（如验证集的误差）来剪掉某些子树。

限制树的深度

- 设定最大深度：通过设置决策树的最大深度，可以限制树的复杂度，从而减少过拟合的风险。这种方法简单直观，但需要合理选择深度参数。
- 设定最小叶节点样本数：当某个节点的样本数小于预设的阈值时，停止该节点的进一步分裂。这可以防止树在只有少量样本的情况下继续生长，从而避免过拟合。

特征选择

- 去除无关特征：只保留对目标变量有显著影响的特征，可以减少模型的复杂度，从而降低过拟合的风险。
- 特征工程：通过对原始数据进行转换和组合，创造出更有信息量的特征。良好的特征工程可以提高模型的性能，同时减少过拟合的可能性。

## 15.**什么是集成学习？集成学习有哪些框架？简单介绍各个框架的常用算法。**

- 集成学习是一种优化手段和策略，通常是结合多个简单的弱分类器来集成模型组，去做更可靠的决策。一般的弱分类器可以是决策树，SVM，kNN等构成，其中的模型可以单独来训练，并且这些弱分类器以某种方式结合在一起去做出一个总体预测。集成学习就是找出哪些弱分类器可以结合在一起，以及如何结合的方法。目前集成学习主要有bagging，boosting，stacking三种：
- bagging：对训练集进行随机子抽样，对每个子训练集构建基模型，对所有的基模型的预测结果进行综合产生最后的预测结果。如果是分类算法，则用多数投票法确定最终类别，如果是回归算法，则将各个回归结果做算术平均作为最终的预测值。常用的bagging算法：[随机森林](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=随机森林&zhida_source=entity)
- boosting：训练过程为阶梯状，基模型按照次序进行训练（实际上可以做到并行处理），先给定一个初始训练数据，训练出第一个基模型，根据基模型的表现对样本进行调整，在之前基模型预测错误的样本上投入更多的关注，然后用调整后的样本训练下一个基模型，重复上述过程N次，将N个基模型进行加权结合，输出最后的结果。常用的算法有GBDT，XGBOOST等。
- stacking：是一种组合分类器的方法，以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输出作为训练集进行再训练(一般用LR进行回归组合），从而得到完整的stacking模型。要得到stacking模型，关键在于如何构造第二层的特征，构造第二层特征的原则是尽可能的避免信息泄露，因此对原始训练集常常采用类似于K折[交叉验证](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=交叉验证&zhida_source=entity)的划分方法。各个基模型要采用相同的Kfold，这样得到的第二层特征的每一折（对应于之前的K折划分）都将不会泄露进该折数据的目标值信息 ，从而尽可能的降低过拟合的风险。

## 16.**简单描述一下模型的偏差和方差？bagging和boosting主要关注哪个？**

- 偏差描述的是预测值与真实值的差距，偏差越大，越偏离真实数据。
- 方差描述的是预测值的变化范围，离散程度，方差越大，数据分布越分散。
- bagging主要关注的是降低方差，boosting主要关注降低偏差。

## 17.**简述一下随机森林的原理，随机森林的构造过程。**

随机森林是bagging算法的代表，使用了CART树作为弱分类器，将多个不同的决策树进行组合，利用这种组合来降低单棵决策树的可能带来的片面性和判断不准确性。对于普通的决策树，是在所有样本特征中找一个最优特征来做决策树的左右子树划分，而随机森林会先通过自助采样的方法（bootstrap）得到N个训练集，然后在单个训练集上会随机选择一部分特征，来选择一个最优特征来做决策树的左右子树划分，最后得到N棵决策树，对于分类问题，按多数投票的准则确定最终结果，对于回归问题，由多棵决策树的预测值的平均数作为最终结果。随机森林的随机性体现在两方面，一个是选取样本的随机性，一个是选取特征的随机性，这样进一步增强了模型的泛化能力。

## 18.**随机森林的优缺点？**

优点：

- 训练可以高度并行化，训练速度快，效率高。
- 两个随机性的引入，使得随机森林不容易过拟合，具有很好的抗噪声能力。
- 由于每次不再考虑全部的特征属性，二是特征的一个子集，所以相对于bagging计算开销更小，效率更高。
- 对于数据的适应能力强，可以处理连续型和离散型的变量，数据无需规范化。
- 可以输出变量的重要程度，被认为是一种不错的降维方法。

缺点：

- 在某些噪声较大的分类问题和或回归问题上容易过拟合。
- 模型的可解释性比较差，无法控制模型内部的运行。
- 对于小数据或者低维数据，效果可能会不太好。

## **19.随机森林为什么不容易过拟合？随机森林的随机怎么体现？**

随机森林之所以不容易过拟合，是因为它通过随机性引入了模型的多样性，同时利用集成学习的优势提高了模型的泛化能力。尽管每棵决策树都可能过拟合，但当它们组合在一起时，过拟合的部分会被相互抵消，从而使得整个模型具有良好的泛化性能。

随机性

- 样本的随机选择：在构建每棵树时，随机森林采用自助采样法从原始数据集中抽取样本，大约有1/3的样本被抽样多次，2/3的样本一次都没有被抽中。这种随机性确保了每棵树的训练集都是不同的，从而增加了模型的多样性。
- 特征的随机选择：在每个节点分裂时，随机森林不是考虑所有的特征，而是从所有特征中随机选择一部分特征，再从中选出最佳的分割特征。这样的做法进一步增加了模型的随机性和多样性。

## 20.随机森林怎么判断特征重要性的？

- 随机森林对于特征重要性的评估思想：判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是基尼指数或袋外数据错误率。
- 基于**[基尼系数](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=基尼系数&zhida_source=entity)**：如果特征X出现在决策树J中的结点M，则计算节点M分枝前后的Gini指数变化量，假设随机森林由N棵树，则计算N次的Gini系数，最后将所有的Gini系数做一个归一化处理就得到了该特征的重要性。
- 基于**袋外数据错误率**：袋外数据指的是每次随机抽取未被抽取达到的数据，假设袋外的样本数为O，将这O个数据作为测试集，代入已生成好的随机森林分类器，得到预测的分类结果，其中预测错误的样本数为X，则袋外数据误差为X/O，这个袋外数据误差记为errOOB1，下一步对袋外数据的特征A加入噪声干扰，再次计算袋外误差errOOB2，假设随机森林由N个分类器，则特征A的重要性为：sum(errOOB2-errOOB1)/N,其依据就是，如果一个特征很重要，那么其变动后会非常影响测试误差，如果测试误差没有怎么改变，则说明特征A不重要。

## 21.**XGBOOST和GBDT的区别在哪里？**

XGBoost和GBDT都是集成学习中用于回归和分类的梯度提升算法。以下是两者的对比介绍：

优化方法

- XGBoost：XGBoost使用二阶泰勒展开，利用了损失函数的一阶和二阶导数信息，这允许算法在优化时更加精确地调整步长和方向。
- GBDT：GBDT仅使用一阶导数信息，即梯度信息进行优化，通常采用简单的[梯度下降法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=梯度下降法&zhida_source=entity)。

正则化

- XGBoost：XGBoost在代价函数中加入了正则项来控制模型的复杂度，如L1和L2正则化，这有助于减少过拟合。
- GBDT：GBDT通常不包括显式的正则项。

缺失值处理

- XGBoost：XGBoost能够自动处理数据中的缺失值，通过分裂点的选择来优化含有缺失值的特征。
- GBDT：GBDT需要对缺失值进行预处理，如填充或删除。

并行计算

- XGBoost：XGBoost支持特征级别的并行计算，显著提高了算法的计算效率。
- GBDT：GBDT通常是串行执行，虽然可以通过一些技术实现并行化，但不如XGBoost高效。

## 22.**为什么XGBOOST要用泰勒展开，优势在哪里？**

xgboost使用了一阶和二阶偏导，二阶导数有利于梯度下降的更快更准，使用泰勒展开取得函数做自变量的二阶导数形式，可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂优化计算，本质上也就把损失函数的选取和模型算法的优化分开来了，这种去耦合增加了xgboost的适用性，使得它按需选取损失函数，既可以用于分类，也可以用于回归。

## 23.**XGBOOST是如何处理缺失值的？**

xgboost为缺失值设定了默认的分裂方向，xgboost在树的构建过程中选择能够最小化训练误差的方向作为默认的分裂方向，即在训练时将缺失值划入左子树计算训练误差，再划入右子树计算训练误差，然后将缺失值划入误差小的方向。

## 24.**XGBOOST的并行化是如何实现的？**

- xgboost的并行不是在tree粒度上的并行，xgboost也是一次迭代完才能进行下一次迭代（第t次迭代的损失函数包含了第t-1次迭代的预测值），它的并行处理是在特征粒度上的，在决策树的学习中首先要对特征的值进行排序，然后找出最佳的分割点，xgboost在训练之前，就预先对数据做了排序， 然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
- 可并行的近似[直方图](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=直方图&zhida_source=entity)算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，[贪心算法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=贪心算法&zhida_source=entity)效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

## 25.**XGBOOST采样时有放回的还是无放回的**

xgboost属于boosting方法的一种，所以采样时样本是不放回的，因而每轮计算样本不重复，另外，xgboost支持子采样，每轮计算可以不使用全部的样本，以减少过拟合。另外一点是xgboost还支持列采样，每轮计算按百分比随机抽取一部分特征进行训练，既可以提高速度又能减少过拟合。

## 26.**XGBOOST的调参步骤是怎样的？**

- 保持learning rate和其他booster相关的参数不变，调节和estimators的参数。learing_rate可设为0.1, max_depth设为4-6之间，min_child_weight设为1，subsample和colsample_bytree设为0.8 ，其他的参数都设为默认值即可。
- 调节max_depth 和 min_child_weight参数，首先，我们先大范围地粗调参数，然后再小范围地微调。
- gamma参数调优
- subsample和colsample_bytree 调优
- 正则化参数调优，选择L1正则化或者L2正则化
- 缩小learning rate，得到最佳的learning rate值

## 27.**LightGBM相比XGBOOST在原理和性能上的差异？**

1.速度和内存上的优化：

- xgboost用的是预排序（pre-sorted）的方法， 空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。
- LightGBM用的是直方图（Histogram）的决策树算法，直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

2.准确率上的优化：

- xgboost 通过level（depth）-wise策略生长树， Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。
- LightGBM通过leaf-wise（best-first）策略来生长树， Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

3.对类别型特征的处理**：**

- xgboost不支持直接导入类别型变量，需要预先对类别型变量作亚编码等处理。如果类别型特征较多，会导致[哑变量](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=哑变量&zhida_source=entity)处理后衍生后的特征过多，学习树会生长的非常不平衡，并且需要非常深的深度才能来达到较好的准确率。
- LightGBM可以支持直接导入类别型变量（导入前需要将字符型转为整数型，并且需要声明类别型特征的字段名），它没有对类别型特征进行[独热编码](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=独热编码&zhida_source=entity)，因此速度比独热编码快得多。LightGBM使用了一个特殊的算法来确定属性特征的分割值。基本思想是对类别按照与目标标签的相关性进行重排序，具体一点是对于保存了类别特征的直方图根据其累计值(sum_gradient/sum_hessian)重排序,在排序好的直方图上选取最佳切分位置。

## 28.**特征工程的一般步骤是什么？什么是特征工程的迭代？**

特征工程常规步骤：

- 数据获取，数据的可用性评估（覆盖率，准确率，获取难度）
- 探索性数据分析，对数据和特征有一个大致的了解，同时进行数据的质量检验，包括缺失值，异常值，重复值，一致性，正确性等。
- 特征处理，包括数据预处理和特征转换两部分，数据预处理主要做清洗工作（缺失值，异常值，错误值，数据格式），特征转换即对连续特征，离散特征，时间序列特征进行转换，便于入模。
- 特征构建，特征构建的目的是找寻与目标变量相关且区分度较好的特征。常用的方法有特征交叉，[四则运算](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=四则运算&zhida_source=entity)，基于业务理解进行头脑风暴构建特征等。
- 特征筛选，大量的特征中选择少量的有用特征，也叫作特征降维，常用的方法有[过滤法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=过滤法&zhida_source=entity)，包装法，嵌入法。

特征工程的迭代:

- 选择特征：具体问题具体分析，通过查看大量的数据和基于对业务的理解，从数据中查找可以提出出数据的关键。
- [设计特征](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=设计特征&zhida_source=entity)：可以自动进行特征提取工作，也可以手工进行特征的构建。
- 选择特征：使用不同的特征构造方法，从多个角度来评判这个特征是否适合放入模型中。
- 计算模型：计算模型在该特征上所提升的准确率。
- 上线测试：通过在线测试的效果来评估特征是否有效。

## 29.**常用的特征工程方法有哪些？**

- 特征处理：数据的预处理包括异常值和缺失值，要根据实际的情况来处理。特征转换主要有标准化，归一化，区间缩放，二值化等，根据特征类型的不同选择合适的转换方法。
- 特征构建：特征之间的四则运算（有业务含义）,基于业务理解构造特征，分解类别特征，特征交叉组合等。
- 特征筛选：过滤法，封装法，嵌入法。

## 30.**在实际的风控建模中怎么做好特征工程？**

- 因为做[风控模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=风控模型&zhida_source=entity)大部分的数据源来自第三方，所以第三方数据的可用性评估非常重要，一方面需要了解这些特征底层的衍生逻辑，判断是否与目标变量相关。另一方面考察数据的覆盖率和真实性，覆盖率较低和真实性存疑的特征都不能使用在模型中。
- 基于金融的数据特点，在特征筛选这个步骤上考量的因素主要有：一个是时间序列上的稳定性，衡量的指标可以是PSI，方差或者IV。一个是特征在样本上覆盖率，也就是特征的缺失率不能太高。另外就是特征的可解释性，特征与目标变量的关系要在业务上要解释的通。
- 如果第三方返回有用户的原始底层数据，例如社保的缴纳记录，运营商的通话/短信记录，则需要在特征衍生上进行特殊处理，基于自身对数据的敏感性和业务的理解，构建具有金融，风险属性的特征，也可以与业务部门进行沟通找寻与业务相关的特征。

## **31.在风控项目中原始数据通常有哪些问题？该如何解决？**

- 一些特征的底层逻辑不清晰，字面上的意思可能与实际的衍生逻辑相悖，这个需要与第三方数据供应商进行沟通，了解清楚特征的衍生逻辑。
- 数据的真实性可能存在问题。比如一个特征是历史总计，但第三方只是爬取了用户近2年的数据，这样的特征就不符合用户的真实情况。所以对数据的真实性校验显得非常重要。
- 有缺失的特征占的比例较高。在进行缺失值处理前先分析缺失的原因，而不是盲目的进行填充，删除等工作。另外也要分析缺失是否有风险属性，例如芝麻分缺失的用户相对来说风险会较高，那么缺失可以当做一个类别来处理。
- 大量多类特征如何使用。例如位置信息，设备信息这些特征类别数较多，如果做亚编码处理会造成[维度灾难](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=维度灾难&zhida_source=entity)，目前常用的方法一个是降基处理，减少类别数，另一个是用xgboost来对类别数做重要性排序，筛选重要性较高的类别再做亚编码处理。

## 32.**在做评分卡或其他模型中，怎么衡量特征(数据)的有用性？**

- 特征具有金融风险属性，且与目标变量的关系在业务上有良好的可解释性。
- 特征与目标变量是高度相关的，衡量的指标主要是IV。
- 特征的准确率，这个需要了解特征的衍生逻辑，并与实际一般的情况相比较是否有异常。
- 特征的覆盖率，一般来说覆盖率要达到70%以上。
- 特征的稳定性，特征的覆盖率，分布，区分效果在时间序列上的表现比较稳定。
- 特征的及时性，最好是能代表用户最近的信用风险情况。

## 33.**缺失值的处理方式有哪些？风控建模中该如何合理的处理缺失？**

- 首先要了解缺失产生的原因，因数据获取导致的缺失建议用填充的方式(缺失率比较低的情况下），因用户本身没有这个属性导致的缺失建议把缺失当做一个类别。另外可以分析缺失是否有风险属性，有的话最好当做一个类别来处理。
- 风控模型对于缺失率的要求比较高，尤其是评分卡。个人认为，缺失率在30%以上的特征建议不要用，缺失率在10%以下的变量可用中位数或随机森林来填充，10%-30%的缺失率建议当做一个类别。对于xgboost和lightgbm这类可以自动处理缺失值的模型可以不做处理。

## **34.如何发现数据中的异常值？对异常值是怎么处理的？**

- 一种是基于统计的异常点检测算法例如极差，[四分位数间距](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=四分位数间距&zhida_source=entity)，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。另一种主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，检测的标准有[欧式距离](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=欧式距离&zhida_source=entity)，绝对距离。
- 对于异常值先检查下是不是数据错误导致的，数据错误的异常作删除即可。如果无法判别异常的原因，要根据实际情况而定，像评分卡会做WOE转换，所以异常值的影响不大，可以不做处理。若异常值的数量较多，建议将异常值归为一类，数量较少作删除也可以。

## **35.对于时间序列特征，连续特征，离散特征这三类是怎么做特征转换的？**

- 时间序列特征：将时间变量的维度进行分离（年/月/日/时/分/秒），或者与位置变量进行结合衍生成新的特征。
- 连续型特征：标准化，归一化，区间缩放，离散化。在评分卡中主要用的是离散化，离散化常用的方法有卡房分箱，决策树分箱，等频和等深分箱。
- 离散型特征：如果类别数不是很多，适合做亚编码处理，对于无序离散变量用独热编码，有序离散变量用顺序编码。如果类别数较多，可用平均数编码的方法。

## **36.如何处理样本不平衡的问题？**

- 在风控建模中出现样本不平衡主要是坏样本的数量太少，碰到这个问题不要急着试各种[抽样方法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=抽样方法&zhida_source=entity)，先看一下坏用户的定义是否过于严格，过于严格会导致坏样本数量偏少，中间样本偏多。坏用户的定义一般基于滚动率分析的结果，不过实际业务场景复杂多样，还是得根据情况而定。
- 确定好坏用户定义是比较合理的之后，先尝试能不能扩大数据集，比如一开始取得是三个月的用户数据，试着将时间线延长来增加数据。因为机器学习是使用现在的数据在整个数据分布上进行估计，因此更多的数据往往能够得到更多的分布信息，以及更好的分布估计。
- 对数据集进行抽样，一种是进行[欠采样](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=欠采样&zhida_source=entity)，通过减少大类的数据样本来降低数据的不平衡，另一种是进行[过采样](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=过采样&zhida_source=entity)，通过增加小类数据的样本来降低不平衡，实际工作中常用SMOTE方法来实现过采样。
- 尝试使用xgboost和lightgbm等对不平衡数据处理效果较好的模型。

## **37.特征衍生的方法有哪些？说说你平时工作中是怎么做特征衍生的？**

常规的特征衍生方法：

- 基于对业务的深入理解，进行头脑风暴，构造特征。
- 特征交叉，例如对类别特征进行交叉相乘。
- 分解类别特征，例如对于有缺失的特征可以分解成是否有这个类别的二值化特征，或者将缺失作为一个类别，再进行亚编码等处理。
- 重构数值量（单位转换，整数小数拆分，构造阶段性特征）
- 特征的四则运算，例如取平均/最大/最小，或者特征之间的相乘相除。

平时工作特征衍生的做法：

- 因为风控模型通常需要好的解释能力，所以在特征衍生时也会考虑到衍生出来的特征是否与目标变量相关。例如拿到运营商的通话记录数据，可以衍生一个"在敏感时间段（深夜）的通话次数占比"，如果占比较高，用户的风险也较大。
- 平常会将大量的时间和精力花在底层数据的衍生上，这个不仅需要对业务的理解，也需要一定的想象力进行头脑风暴，即使衍生出来的特征90%都效果不佳，但只要剩下的10%是好的特征，那对于模型效果的提升是很显著的。
- 对于评分卡来说，特征需要好的解释能力，所以一些复杂的衍生方法，像特征交叉，log转换基本不会用到。但如果是xgboost等复杂模型，进行特征交叉等方法或许有比较好的效果。

## **38.特征筛选的作用和目的？筛选的特征需要满足什么要求？**

作用和目的：

- 简化模型，增加模型的可解释性， 降低模型过拟合的风险。
- 缩短模型的训练时间。
- 避免维度灾难。

筛选特征满足的要求：

具有良好的区分能力。

- 可解释性好，与目标变量的关系在业务上能解释的通。
- 在时间序列上有比较好的稳定性。
- 特征的用户覆盖率符合要求。

## **39.特征筛选的方法有哪些？每种方法的优缺点？实际工作中用到了哪些方法？**

Filter（过滤法）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。

- 相关系数，方差（适用于连续型变量），[卡方检验](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=卡方检验&zhida_source=entity)（适用于类别型变量），[信息熵](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=信息熵&zhida_source=entity)，IV。实际工作中主要基于IV和相关性系数（皮尔逊系数）。
- 优点：算法的通用性强；省去了分类器的训练步骤，[算法复杂性](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=算法复杂性&zhida_source=entity)低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。
- 缺点：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。

Wrapper（封装法）：封装式特征选择是利用学习算法的性能评价特征子集的优劣。因此，对于一个待评价的特征子集，Wrapper方法需要训练一个分类器，根据分类器的性能对该特征子集进行评价。

- 方法有完全搜索（递归消除法），[启发式搜索](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=启发式搜索&zhida_source=entity)（前向/后向选择法，逐步选择法），随机搜索（训练不同的特征子集）。实际工作中主要用到启发式搜索，例如评分卡的逐步逻辑回归。
- 优点：相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。
- 缺点：Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。

Embedded（嵌入法）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

- 一种是基于惩罚项，例如[岭回归](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=岭回归&zhida_source=entity)，lasso回归，L1/L2正则化。另一种是基于树模型输出的特征重要性，在实际工作中较为常用，可选择的模型有随机森林，xgboost，lightgbm。
- 优点：效果最好速度最快，模式单调，快速并且效果明显。
- 缺点：如何参数设置， 需要对模型的算法原理有较好的理解。

## **40.简单介绍一下风控模型常用的评估指标。**

- [混淆矩阵](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=混淆矩阵&zhida_source=entity)指标：精准率，查全率，假正率。当模型最后转化为规则时，一般用这三个指标来衡量规则的有效性。要么注重精准率，要么注重查全率，两者不可兼而得之。
- ROC曲线和AUC值，ROC曲线是一种对于查全率和假正率的权衡，具体方法是在不同阈值下以查全率作为纵轴，假正率作为横轴绘制出一条曲线。曲线越靠近左上角，意味着越多的正例优先于负例，模型的整体表现也就越好。AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。在对角线（随机线）左边的点上TPR总大于FPR，意为正例被判为正例的概率大于负例被判为正例的概率。从另一个角度看，由于画ROC曲线时都是先将所有样本按分类器的预测概率排序，所以AUC反映的是分类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。
- KS：用于区分预测正负样本分隔程度的评价指标，KS越大，表示模型能将好坏样本区分开的程度越大。KS的绘制方法是先将每个样本的预测结果化为概率或者分数，将最低分到最高分（分数越低，坏的概率越大）进行排序做样本划分，横轴就是样本的累计占比，纵轴则是好坏用户的累计占比分布曲线，KS值为两个分布的最大差值（绝对值）。KS值仅能代表模型的区隔能力，KS不是越高越好，KS如果过高，说明好坏样本分的过于开了，这样整体分数（概率）就是比较极端化的分布状态，这样的结果基本不能用。
- 基尼系数：其横轴是根据分数（概率）由高到低累计的好用户占总的好用户的比例，纵轴是分数（概率）从高到低坏用户占总的坏用户的比例。由于分数高者为低风险用户，所以累计坏用户比例的增长速度会低于累计好用户比例，因此，基尼曲线会呈现向下弯曲的形式，向下突出的半月形的面积除以下方三角形的面积即是基尼系数。基尼系数越大，表示模型对于好坏用户的区分能力越好。

## **41.什么是模型的欠拟合和过拟合？**

- 欠拟合指的是模型没有很好的捕捉到数据特征，不能很好的拟合数据。
- 过拟合指的是模型把数据学习的太彻底，以至于把噪声数据学习进去了，这样模型在预测未知数据时，就不能正确的分类，模型的泛化能力太差。

## **42.如何判断模型是否存在过拟合或欠拟合？对应的解决方法有哪些？**

- 判断模型是否存在过拟合/欠拟合主要用学习曲线，学习曲线指的是通过画出不同训练集大小时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高（过拟合）或偏差过高（欠拟合）。当训练集和测试集的误差收敛但却很高时，即为欠拟合，当训练集和测试集的误差之间有大的差距时，为过拟合。
- 解决欠拟合的方法：增加效果好的特征，添加多项式特征，减小正则化参数等。
- 解决过拟合的方法：使用更多的数据，选择更加合适的模型，加入正则项等。

## **43.什么是正则化？什么是L1正则化和L2正则化？**

- 正则化是在模型的loss function的基础上，加上了一些正则化项或者称为模型复杂度惩罚项，它会向学习算法略微做些修正，从而让模型能更好地泛化。这样反过来能提高模型在不可见数据上的性能。
- L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解，所以L1正则化会趋向于产生少量的特征。
- L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），所以L2正则化会使特征的解趋近于0，但不会为0。

## **44.正则化为什么可以防止过拟合？**

最简单的解释是正则化对模型参数添加了先验，在数据少的时候，[先验知识](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=先验知识&zhida_source=entity)可以防止过拟合。举个例子：抛一枚硬币5次，得到的全是正面，则得出结论：正面朝上的概率为1，这类似于模型的过拟合，如果加上硬币朝上的概率是0.5的先验，结果就不会这么离谱，这就是正则。

## **45.什么是交叉验证？交叉验证的目的是什么？有哪些优点？**

交叉验证概念：

交叉验证，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓"交叉"。　

交叉验证的目的：

评估给定算法在特定数据集上训练后的泛化性能，比单次划分训练集和测试集的方法更加稳定，全面。

交叉验证的优点：

- 如果只是对数据随机划分为训练集和测试集，假如很幸运地将难以分类的样本划分进训练集中，则在测试集会得出一个很高的分数，但如果不够幸运地将难以分类的样本划分进测试集中，则会得到一个很低的分数。所以得出的结果随机性太大，不够具有代表性。而交叉验证中每个样本都会出现在训练集和测试集中各一次，因此，模型需要对所有样本的泛化能力都很好，才能使其最后交叉验证得分，及其平均值都很高，这样的结果更加稳定，全面，具有说服力。
- 对数据集多次划分后，还可以通过每个样本的得分比较，来反映模型对于训练集选择的敏感性信息。
- 对数据的使用更加高效，可以得到更为精确的模型。

## **46.交叉验证常用的方法有哪些？**

- 标准K折交叉验证：K是自定义的数字，通常取5或10，如果设为5折，则会训练5个模型，得到5个精度值。
- 分层K折交叉验证：如果一个数据集经过标准K折划分后，在测试集上只有一种类别，则无法给出分类器整体性能的信息，这种情况用标准K折是不合理的。而在分层K折交叉验证中，每个折中的类别比例与整个数据集类别比例相同，这样能对泛化性能做出更可靠的估计。
- 留一法交叉验证：每次划分时，把单个数据点作为测试集，如果数据量小，能得到更好的估计结果，数据量很大时则不适用。
- 打乱划分交叉验证：每次划分数据时为训练集取样train_size个点，为测试集取样test_size个点，将这一划分划分方法重复n_splits次。这种方法还允许每次迭代中使用部分数据，可通过设置train_size和test_size之和不为0来实现，用这种方法对数据进行二次采样可能对大型数据上的试验很用用。另外也有分层划分的形式（ StratifiedShuffleSplit），为分类任务提供更可靠的结果。
- 分组交叉验证：适用于数据中的分组高度相关时，以group数组作为参数，group数组表示数据中的分组，在创建训练集和测试集的时候不应该将其分开，也不应该与类别标签弄混。

## 47.**互联网金融场景下的的风控模型种类？**

- 获客阶段：用户响应模型，风险预筛选模型。
- 授信阶段：申请评分模型，反欺诈模型，[风险定价](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=风险定价&zhida_source=entity)模型，收益评分模型。
- 贷后阶段：行为评分模型，交易欺诈模型，客户流失模型。
- 催收阶段：早期催收模型，晚期催收模型。

## 48.**简单描述一下风控建模的流程？、**

- 前期准备工作：不同的模型针对不同的业务场景，在建模项目开始前需要对业务的逻辑和需求有清晰的理解，明确好模型的作用，项目周期时间和安排进度，以及模型效果的要求。
- 模型设计：包括模型的选择（评分卡还是集成模型），单个模型还是做模型的细分，是否需要做拒绝推论，观察期，表现期的定义，好坏用户的定义，数据的获取途径等都要确定好。
- 数据拉取及清洗：根据观察期和表现期的定义从数据池中取数，并进行前期的数据清洗和稳定性验证工作，数据清洗包括用户唯一性检查，缺失值检查，异常值检查等。稳定性验证主要考察变量在时间序列上的稳定性，衡量的指标有PSI，平均值/方差，IV等。
- 特征工程：主要做特征的预处理和筛选，如果是评分卡，需要对特征进行离散化，归一化等处理，再对特征进行降维，降维的方法有IV筛选，相关性筛选，显著性筛选等。另外会基于对业务的深入理解做特征构造工作，包括特征交叉，特征转换，对特征进行四则运算等。
- 模型建立和评估：选择合适的模型，像评分卡用逻辑回归，只需要做出二分类预测可以选择xgboost等集成模型，模型建好后需要做模型评估，计算AUC,KS，并对模型做交叉验证来评估泛化能力及模型的稳定性。
- 模型上线部署：在风控后台上配置模型规则，对于一些复杂的模型还得需要将模型文件进行转换，并封装成一个类，用Java等其他形式来调用。
- 模型监控：前期主要监控模型整体及变量的稳定性，衡量标准主要是PSI，并每日观察模型规则的拒绝率与线下的差异。后期积累一定线上用户后可评估线上模型的AUC,KS，与线下进行比较，衡量模型的线上的实际效果。

## 49.**评分卡，集成模型在线上是如何部署的？**

- 评分卡的部署较为简单，因为评分卡将变量映射到了一个个区间及得分，所以在普通的风控决策引擎上就可配置。
- 像一些比较复杂的模型，例如xgboost和lightgbm，一般是将模型文件转换为pmml格式，并封装pmml，在风控后台上上传pmml文件和变量参数文件，并配置好模型的阈值。python模型和R模型都可以用这种方式来部署。

## 50.**对于金融场景，稳定胜于一切，那在建模过程中如何保证模型的稳定性？**

- 在数据预处理阶段可以验证变量在时间序列上的稳定性，通过这个方法筛掉稳定性不好的变量，也能达到降维的目的。筛选的手段主要有：计算月IV的差异，观察变量覆盖率的变化，两个时间点的PSI差异等。
- 异常值的检查，剔除噪声，尤其对于逻辑回归这种对于噪声比较敏感的模型。
- 在变量筛选阶段剔除与业务理解相悖的变量，如果是评分卡，可以剔除区分度过强的变量，这种变量一般不适合放入模型中，否则会造成整个模型被这个变量所左右，造成模型的稳定性下降，过拟合的风险也会增加。
- 做交叉验证，一种是时间序列上的交叉验证，考察模型在时间上的稳定性，另一种是K折随机交叉验证，考察模型的随机稳定性。
- 选择稳定性较好的模型，例如随机森林或xgboost这类泛化能力较好的模型。

## 51.**模型转化为规则后决策点（cutoff点）怎么设定？**

- 规则只是判断用户好坏，而不会像模型会输出违约概率，所以设定决策点时需要考虑到规则的评估指标（精准率，查全率，误伤率，拒绝率），一般模型开发前会设定一个预期的拒绝率，在这个拒绝率下再考量精确率，查全率和误伤率的取舍，找到最佳的平衡点。
- 好的模型能接受更多的好用户，拒绝掉更多的坏用户，也就是提高好坏件比例，所以可事先设定一个预期目标的好坏件比例来选择最佳的决策点。

## **52.模型上线后是怎么监控的？**

前期监控（模型上线后一个月内）：

- 模型最后设定cutoff点后可以得出模型的拒绝率（线下拒绝率）, 上线后需要比较模型每日的拒绝率与线下拒绝率。如果两者差异较大，说明线上的用户与建模的用户分布有很大差异，原因可能是没做拒绝推断，或者用户属性随着时间发生了偏移。
- 监控模型整体的稳定性，通常用PSI来衡量两个时间点的差异程度。模型的稳定性是一个需要长期观察的指标，可绘制月/周PSI变化趋势图来分析稳定性的变化，从中可以发现用户是否随着时间推移属性发生了变化，以便及时对模型做出合理的调整。
- 变量稳定度分析，目的是如果模型的稳定性不好，可利用变量稳定度分析来了解是哪些变量造成的。对于不稳定的变量要分析其原因，并对模型做出调整，弃用不稳定的变量或者找其他变量来替换。

后期监控（用户表现出了好坏程度）：

- 此时已积累了一些线上的好坏用户，可做模型的线上效果的评估，评估的指标有AUC, KS, 基尼系数，如果模型的线下效果好，但线上效果却不理想，这个模型是要做优化的。
- 好坏用户的评分分布。绘制线上好坏用户的评分分布图，如果符合期望（高分段好用户占比多，低分段坏用户占比多），则说明模型的线上的区隔能力较好。
- 变量鉴别力分析。用线上的好坏用户来计算变量的IV值，评价变量的预测能力，预测能力不好的变量可以考虑弃用。

## 53.**当模型上线后发现稳定性不佳，或者线上的区分效果不好，你是怎么对模型作调整的？**

- 模型稳定性不佳先检查当初建模时有没有考量过特征的稳定性，在模型前期监控一般会做变量的稳定性分析，如果发现稳定性不佳的变量，考虑弃用或用其他变量替代。另外可以分析下线上用户和建模用户的分布差异，考虑在建模时增加拒绝推断的步骤，让建模样本的分布更加接近于实际整体的申请用户。
- 线上的效果不好可以从变量角度分析，做一下变量鉴别度分析，剔除掉效果不好的变量，挖掘新的变量入模。如果一个模型已上线较长的时间，用户的属性也慢慢发生偏移，建议重新取数做一个新的模型替代旧模型。

## **54.对于高维稀疏特征，或者是弱特征，你是怎么处理的？**

- 对于高维稀疏特征，逻辑回归的效果要比GBDT好。这是由于逻辑回归的正则项是对特征权重的惩罚，以至于特征的权重不至于过大，而树模型的惩罚项主要是深度和叶子节点数目，而对于高维稀疏特征,10000个样本可能9990个值是0，那只需要一个节点就可以划分9990和剩下的10个样本，可见惩罚项之小，所以GBDT对于高维稀疏特征很容易过拟合。平时工作中如果用的是逻辑回归评分卡，则可以对稀疏特征进行离散化，离散成值为0或不为0，再用woe进行编码。而如果使用xgboost等集成模型，最好还是不要用高维的稀疏特征。
- 弱特征指的是与目标变量关系不大的特征，或者是区分能力较弱的特征。在大数据风控中弱特征的种类很多，包括社交，通话，位置等信息，而且建模时弱特征会多达数百个。如果是用评分卡建模，弱特征一般会被舍弃掉，因为评分卡的入模特征数不宜过多，一般在15个以下，所以要找寻比较强的特征。而对于xgboost等模型，本身对数据的要求不是很高，并且精度好，一些弱特征进行交叉组合或许能给模型带来不错的效果。

## 55.**如何根据风险因素对用户分层，构建客群差异化的模型？**

做客群差异化模型之前最好做一下用户画像，在风控领域中做用户画像的目的是：

- 系统性的梳理用户群体，找到异同点对用户进行划分群体，分类的维度很多，可以是静态属性，购买偏好，也可以是褥羊毛党等风险属性。
- 便于更深刻的理解业务，理解用户需求，风控离不开业务，只有深刻理解业务后，才能发现更多潜在的风险。
- 便于后续的数据挖掘，了解坏用户的行为特征，并且根据用户特征做[关联规则](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=关联规则&zhida_source=entity)分析。
- 对不同类型的用户，做针对性的风控规则和风控模型。

平常工作中的做法：

- 对用户做静态属性的划分，比如按性别，年龄，收入，职业等。例如刚毕业工作的年轻人和收入比较稳定的中年人，他们的借款需求，风险程度就不一样，可以先对用户群体做这样的划分，再对每个群体单独建立模型。
- 根据用户风险属性做差异化模型，例如对手机分期业务做一个套现风险模型，挖掘套现风险属性，目标变量变成是否为套现用户。

## 56.**额度，利率的风险定价模型你是如何设计的？**

- 首先做风险定价模型需要熟悉产品的属性和特点，像小额现金贷和大额分期贷两种产品的额度定价逻辑就不同。另外也要了解产品的盈利模式和预期的利润，这点需要与业务部门做好沟通，通常关于额度，利率也是业务或者产品制定的。
- 风险定价模型一般采用评分卡模型，最后设定cutoff点后对通过的用户进行风险等级划分，对于风险高的用户给的额度较低，或者利率较高。一般来说中低额度的用户占大部分，高额度用户占小部分，最后可以得出一个平均额度或利率，这个值事先可以根据预期的利润/资损来计算。

## 57. **风控流程中不同环节的评分卡是怎么设计的？**

- 申请评分A卡用在贷前审核阶段，主要的作用是决定用户是否准入和对用户进行风险定价（确定额度和利率），用到的数据是用户以往的信用历史，多头借贷，消费记录等信息，并且做A卡一般需要做拒绝推断。A卡一般预测用户的首笔借款是否逾期，或者预测一段时间内是否会逾期，设计的方式也多种多样，有风险差异化评分卡，群体差异化评分卡，或者做交叉评分卡等。
- 行为B卡主要用在借贷周期较长的产品上，例如手机分期。作用一是防控贷中风险，二是对用户的额度做一个调整。用到的数据主要是用户在本平台的登录，浏览，消费行为数据，还有借还款，逾期等借贷表现数据。
- 催收C卡主要是对逾期用户做一个画像分析，通过深度挖掘用户特征，对逾期用户进行分群，做智能催收策略等。