# K近邻(kNN)
k近邻方法是一种惰性学习算法，可以用于回归和分类，它的主要思想是投票机制，对于一个测试实例$x_j$, 我们在有标签的训练数据集上找到和最相近的k个数据，用他们的label进行投票，分类问题则进行表决投票，回归问题使用加权平均或者直接平均的方法。
## 整体介绍
正所谓物以类聚,人以群分,kNN就是利用这个思想的一种学习算法, 对于每一个预测的实例,找打和它相近的k个实例,用这k个实例的平均水平表示这个待预测的实例. 
如果是一个预测一个人平均收入的问题,我们都知道只要知道他的k个朋友的相关收入求平均值即可. KNN就是这么做的,只是需要指定K,并且怎么判断"朋友".

伪代码:     
> 输入:训练数据
> $$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_n)\}$$
> 其中$x_i \in R^n$,是实例的特征向量，$y_i \in Y = \{c_1,c_2,...,c_K\}$,表示类别，      
> 输出: 实例x所属的类别     
> 1. 根据跟定的距离度量的方法，在T中找到和x最邻近的k个点，记作x的邻域，$N_k(x)$
> 2. 在$N_k(x)$中使用多数表决规则，绝对x的类别y:
> $$y=argmax_{c_j} \sum_{x_i \in N_k(x)}I(y_i=c_j)$$
> 3. 对于回归问题,得到y
> $$y= \frac{1}{k} \sum_{x_i \in N_k(x)}y_i$$
> 其中$i=1,2,...,N; j=1,2,...,K$.

## 核心公式
1. 距离的度量(怎么判断朋友)     
对于两个向量$(x_i, x_j)$,一般使用$L_p$距离进行计算。
假设特征空间$X$是n维实数向量空间$R^n$, 其中，$x_i,x_j \in X$, 
$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})，x_j=(x_j^{(1)},x_j^{(2)},...,x_j^{(n)})$， 

$x_i,x_j$的$L_p$距离定义为:  
> $$L_p(x_i,x_j) = \left( \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p \right) ^ {\frac{1}{p}}$$
> 这里的$p \geq 1$. 当p=2时候，称为欧氏距离(Euclidean distance), 有
> $$L_2(x_i,x_j) = \left( \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2 \right) ^ {\frac{1}{2}}$$
> 当p=1时候，称为曼哈顿距离(Manhattan distance), 有
> $$L_1(x_i,x_j) = \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}| $$
> 当$p=\infty$时候，称为极大距离(infty distance), 表示各个坐标的距离最大值， 有
> $$L_p(x_i,x_j) = \max_{l}{n}|x_i^{(l)}-x_j^{(l)}|$$

2. k值的选择
> kNN中的k是一个超参数，需要我们进行指定，一般情况下这个k和数据有很大关系，都是交叉验证进行选择，但是建议使用交叉验证的时候，$k \in [2,20]$, 使用交叉验证得到一个很好的k值。    
> k值还可以表示我们的模型复杂度，当k值越小意味着模型复杂度表达，更容易过拟合，(用极少树的样例来绝对这个预测的结果，很容易产生偏见，这就是过拟合)。我们有这样一句话，k值越大学习的估计误差越小，但是学习的近似误差就会增大.  

3. 怎么理解 "k值越大学习的估计误差越小，但是学习的近似误差就会增大"
> 估计误差表示最后的结果，k值大，集百家所长，更可能得到准确的值，表示估计的准确，则误差就小；但是我们的估计的时候，在学习过程中，使用最相近的k个实例进行估计，每一个值都会和预测的x有一个近似误差，k越大则误差的总和就越大。        

4. 多数表决等价于经验风险最小化
> 《统计学习方法》第40页 *分类决策规则*. **多说一句，既然要求误差，那就写出误差损失函数，剩下的就是公式恒等变化了**.

5. KD树
> kNN，从上述算法中，我们可以看到主要是从训练数据中，知道k个相近实例，但是每次都要便利这个数据集合，主要的问题就是速度慢. 这时候就出现了加速查找的数据结构，其中之一就是kd 树.

构造kd 树
> kd树是一种对k维空间中的实例店进行存储以便能够进行快速检索的数据结构. kd树是一个二叉树，表示对k维空间的一个划分. 构造kd树就是不断用**垂直于坐标轴的超平面** (*一般用每一个维度的中位数来表示*) 将k维空间划分，构成一系列的k维超矩形区域.  
 
**伪代码:** (from wikipedia)
```
function kdtree (list of points pointList, int depth)
{
    // Select axis based on depth so that axis cycles through all valid values
    var int axis := depth mod k;
        
    // Sort point list and choose median as pivot element
    select median by axis from pointList;
        
    // Create node and construct subtree
    node.location := median;
    node.leftChild := kdtree(points in pointList before median, depth+1);
    node.rightChild := kdtree(points in pointList after median, depth+1);
    return node;
} 
```
基于kd树查找最近邻
> 构造kd树的目的就是快速查找最近邻和k近邻，这里我们给出二维的列子，这里例子来自与书中和
> wiki百科，我尝试说明百这几张图的运行原理.     
> 数据 T = {(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)}       
> 构造kd树，二叉树和空间角度的划分图，再次注意每一次用的这个维度对应的所有这个空间中的中位数.
> <center><img src="https://upload.wikimedia.org/wikipedia/commons/2/25/Tree_0001.svg" border="0" ><br>二叉树划分图 </center>
> 根据x维度有{2,4,5,7,8,9}: 中位数是7，因此(7,2)作为根节点，x < 7,在左子树，其他在右子树。依次递归构造左右子树，下一次根据维度y，之后根据维度x. 注意这里的k是维度，n 可能大学k，故要每次对k取余数. （这个k不是kNN中的k）.               
>   
> 下面的空间划分图，表示空间上的显示格式. 其实整体的搜索是在这个空间上进行的. 
> <center><img src="https://upload.wikimedia.org/wikipedia/commons/b/bf/Kdtree_2d.svg" border="0" ><br>空间划分图 </center>

> 下面用一个gif图来表示搜索最近邻的过程,简单来说就是一句话，根据构造过程，从头到尾左右二分，在这个过程中记录下来最近的点。从这个图中，我们可以看到，我们目标实例target用四角星表示，
> <center><img src="https://upload.wikimedia.org/wikipedia/commons/9/9c/KDTree-animation.gif" border="0" ><br>空间划分图 </center>

> 1. 二叉树搜索，依次到叶子节点D，路劲是A - B - D. 这个过程中最近的点是B，作为最有候选集合.
> 2. 我们以target为圆心，target到B的距离为半径，画圆，我们发现，B的另外的部分与圆相交，这表示，可能存在更近的候选点在另半部分，如果不存在相交，此时的候选点B就是最近点。
> 3. 如果存在和其他空间相交，则将搜索空间上升为其父节点，用target和其父节点距离作圆，如果依然和其他空间相交，继续回溯搜索，直到不想交或者全部搜索完毕找到候选点.
> 4. 上面介绍的是最近邻查找，如何查找k近邻？
> + 使用各最大堆数据结果，维护k大小的最大堆，从根节点开始如果堆的大小不足k，就候选集如果
> + 如果大小为k，就比较堆顶元素和当前元素的距离大小，如果当前小于堆顶距离就进行替换，
> + 之后以堆顶元素为中心，就编程了找最近邻问题。最后返回结果

## 算法十问
1. 简述一下KNN算法的原理
> KNN算法利用训练数据集对特征向量空间进行划分。KNN算法的核心思想是在一个含未知样本的空间，可以根据样本最近的k个样本的数据类型来确定未知样本的数据类型。
该算法涉及的3个主要因素是：**k值选择，距离度量，分类决策**。

2. 如何理解kNN中的k的取值？
> 在应用中，k值一般取比较小的值，并采用交叉验证法进行调优。

1. 在kNN的样本搜索中，如何进行高效的匹配查找？
> 线性扫描(数据多时，效率低)
构建数据索引——Clipping和Overlapping两种。前者划分的空间没有重叠，如k-d树；后者划分的空间相互交叠，如R树。（对R树了解很少，可以之后再去了解）
4. KNN算法有哪些优点和缺点？
> 优点：算法思想较简单，既可以做分类也可以做回归；可以用于非线性分类/回归；训练时间复杂度为O(n)；准确率高，对数据没有假设，对离群点不敏感。     
> 缺点：计算量大；存在类别不平衡问题；需要大量的内存，空间复杂度高。

5. 不平衡的样本可以给KNN的预测结果造成哪些问题，有没有什么好的解决方式？
> 输入实例的K邻近点中，大数量类别的点会比较多，但其实可能都离实例较远，这样会影响最后的分类。       
> 可以使用权值来改进，距实例较近的点赋予较高的权值，较远的赋予较低的权值。

6. 为了解决KNN算法计算量过大的问题，可以使用分组的方式进行计算，简述一下该方式的原理。
> 先将样本按距离分解成组，获得质心，然后计算未知样本到各质心的距离，选出距离最近的一组或几组，再在这些组内引用KNN。     
> 本质上就是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本，该方法比较适用于样本容量比较大时的情况。

## 面试真题
1. KD树？怎么构建的？
> kd树是对数据点在k维空间中划分的一种数据结构，主要用于多维空间关键数据的搜索。本质上，kd树就是一种平衡二叉树。

2. K-Means与KNN有什么区别
> KNN
+ KNN是分类算法 
+ 监督学习 
+ 喂给它的数据集是带label的数据，已经是完全正确的数据
+ 没有明显的前期训练过程，属于memory-based learning	
+ K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c

> K-Means
+ 1.K-Means是聚类算法 
+ 2.非监督学习 
+ 3.喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序
+ 有明显的前期训练过程
+ K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识
 	 
**相似点**：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。

3. KD树改进   
> Kd-tree在维度较小时（例如：K≤30），算法的查找效率很高，然而当Kd-tree用于对高维数据（例如：K≥100）进行索引和查找时，就面临着维数灾难（curse of dimension）问题，查找效率会随着维度的增加而迅速下降。通常，实际应用中，我们常常处理的数据都具有高维的特点，例如在图像检索和识别中，每张图像通常用一个几百维的向量来表示，每个特征点的局部特征用一个高维向量来表征（例如：128维的SIFT特征）。因此，为了能够让Kd-tree满足对高维数据的索引，Jeffrey S. Beis和David G. Lowe提出了一种改进算法——Kd-tree with BBF（Best Bin First），该算法能够实现近似K近邻的快速搜索，在保证一定查找精度的前提下使得查找速度较快。

> 在介绍BBF算法前，我们先来看一下原始Kd-tree是为什么在低维空间中有效而到了高维空间后查找效率就会下降。在原始kd-tree的最近邻查找算法中（第一节中介绍的算法），为了能够找到查询点Q在数据集合中的最近邻点，有一个重要的操作步骤：**回溯**，该步骤是在未被访问过的且与Q的超球面相交的子树分支中查找可能存在的最近邻点。随着维度K的增大，与Q的超球面相交的超矩形（子树分支所在的区域）就会增加，这就意味着需要回溯判断的树分支就会更多，从而算法的查找效率便会下降很大。 

> 从上述标准的kd树查询过程可以看出其搜索过程中的“回溯”是由“查询路径”决定的，并没有考虑查询路径上一些数据点本身的一些性质。一个简单的改进思路就是将“查询路径”上的结点进行排序，如按各自分割超平面（也称bin）与查询点的距离排序，也就是说，回溯检查总是从优先级最高（Best Bin）的树结点开始。

**bbf的算法**:      
输入：kd树，查找点x.     
输出：kd树种距离查找点最近的点以及最近的距离
1. 若kd树为空，则设定两者距离为无穷大，返回；如果kd树非空，则将kd树的根节点加入到优先级队列中；
2. 从优先级队列中出队当前优先级最大的结点，计算当前的该点到查找点的距离是否比最近邻距离小，如果是则更新最近邻点和最近邻距离。如果查找点在切分维坐标小于当前点的切分维坐标，则把他的右孩子加入到队列中，同时检索它的左孩子，否则就把他的左孩子加入到队列中，同时检索它的右孩子。这样一直重复检索，并加入队列，直到检索到叶子节点。然后在从优先级队列中出队优先级最大的结点；
3. 重复1和1中的操作，直到优先级队列为空，或者超出规定的时间，返回当前的最近邻结点和距离。

# 参考
1. https://blog.csdn.net/weixin_44915167/article/details/89315734 
2. https://www.cnblogs.com/nucdy/p/6349172.html
3. https://blog.csdn.net/v_july_v/article/details/8203674 
4. https://blog.csdn.net/junshen1314/article/details/51121582
5. https://blog.csdn.net/lhanchao/article/details/52535694 
6. https://blog.csdn.net/fool_ran/article/details/85246432
7. 李航 统计学习方法


