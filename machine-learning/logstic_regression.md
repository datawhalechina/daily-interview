# Logistics Regression
作者：吕雪杰
# 1、直观解释
逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达到将数据二分类的目的。
核心公式
对于给定的数据集$(x_i,y_i)^N_{i=1},y \in {0,1}$
$$ p(y=1|x)=\frac{e^{w^T x+b}}{1+e^{w^T x+b}} $$
$$ p(y=0|x)=\frac{1}{1+e^{w^T x+b}} $$
设$g(x)=p(y=1|x),1-g(x)=p(y=0|x)$
似然函数为
$$ \prod^N_{i=1} [g(x_i)] [1-g(x_i)]^{1-y_i} $$
对数似然函数为
$$ L(w)=\Sigma ^N_{i=1} [y_ilog(x_i)+(1-y_i)log(1-g(x_i))] $$
$$ =\Sigma^N_{i=1}[y_ilog \frac {g(x_i)}{1-g(x_i)} + log(1-g(x_i))] $$
$$ =\Sigma^N_{i=1}[y_i (w \ast x_i)-log(1+\exp(w \ast x_i))] $$
对L(w)求最大值，得到w的估计值,常用梯度下降和牛顿法解决
# 2、算法十问
1、LR 和线性回归的区别?  
  >损失函数：线性模型是平方损失函数，而逻辑回归则是似然函数。LR是分类问题，线性回归是回归方法。  

2、逻辑回归中为什么使用对数损失而不用平方损失?   
 > 对于逻辑回归，这里所说的对数损失和极大似然是相同的。 不使用平方损失的原因是，在使用 Sigmoid 函数作为正样本的概率时，同时将平方损失作为损失函数，这时所构造出来的损失函数是非凸的，不容易求解，容易得到其局部最优解。 而如果使用极大似然，其目标函数就是对数似然函数，该损失函数是关于未知参数的高阶连续可导的凸函数，便于求其全局最优解。   

3、为什么逻辑回归比线性回归要好？   
 > 逻辑回归和线性回归首先都是广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在0,1间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好   

4、逻辑回归的求解方法
 >假设label={-1,+1},则$$p(y=1 | x) = h_{\omega} (x)\tag{1}$$
$$p(y=-1 | x) = 1 - h_{\omega} (x)\tag{2}$$
对于sigmoid函数，有以下特性，
$$h(-x) = 1 - h(x)$$
所以（1）（2）式子可表示为
$$p(y|x) = h_\omega(yx)$$
同样，我们使用MLE作估计， 
$$\begin{aligned}
L(\omega)&=  \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\
&=  \prod_{i=1}^{m} h_\omega(y_i x_i)\\
&= \prod_{i=1}^{m} \frac{1}{1+e^{-y_iwx_i}}
\end{aligned}$$
对上式取对数及负值，得到损失为： 
$$\begin{aligned}
-\log L(\omega)&= -\log \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\
&=  -\sum_{i=1}^{m} \log p(y_i | x_i; \omega)  \\
&=  -\sum_{i=1}^{m} \log \frac{1}{1+e^{-y_iwx_i}}\\
&=  \sum_{i=1}^{m} \log(1+e^{-y_iwx_i})\\
\end{aligned}$$
即对于每一个样本，损失函数为：
$$L(\omega)=\log(1+e^{-y_iwx_i}) $$

5、LR为什么使用sigmoid函数？
 >https://blog.csdn.net/qq_19645269/article/details/79551576
6、如果label={-1, +1}，给出LR的损失函数？

 >如果在损失函数最终收敛的情况下，有很多特征高度相关也不会影响分类器的效果。对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

7、LR如何进行并行计算？
 >http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html
 >
8、逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？
 >如果在损失函数最终收敛的情况下，有很多特征高度相关也不会影响分类器的效果。对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

9、带有L1正则项的逻辑LR如何进行参数更新？
 >请参考西瓜书(周志华，机器学习)，P252
 >
10、逻辑斯特回归是否要对特征进行离散化，以及为什么？
  >在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：离散特征的增加和减少都很容易，易于模型的快速迭代；稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

# 3、面试真题
1.LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？
2.LR和SVM有什么不同吗
 >+ LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）
 > + 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。
 区别：
  >+ LR是参数模型，SVM是非参数模型。
  >+ 从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
  >+ SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
  >+ 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
  >+ logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

3.写一下，LR的损失函数
4.当用lr时，特征中的某些值很大，意味着这个特征重要程度很高？
